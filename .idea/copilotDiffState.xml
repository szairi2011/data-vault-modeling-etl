<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/src/main/scala/bronze/RawVaultSchema.scala">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/main/scala/bronze/RawVaultSchema.scala" />
              <option name="originalContent" value="package bronze&#10;&#10;/**&#10; * ========================================================================&#10; * RAW VAULT SCHEMA - DATA VAULT 2.0 TABLE DEFINITIONS&#10; * ========================================================================&#10; *&#10; * PURPOSE:&#10; * Defines and creates all Data Vault 2.0 tables in the Bronze layer&#10; * (Raw Vault) using Apache Iceberg table format.&#10; *&#10; * DATA VAULT 2.0 ENTITIES:&#10; * - Hubs: Business entities (Customer, Account, Transaction, Transaction Item)&#10; * - Links: Relationships between hubs (Customer-Account, Transaction-Item)&#10; * - Satellites: Descriptive attributes with full history tracking&#10; *&#10; * ICEBERG TABLE BENEFITS:&#10; * - ACID transactions (atomic writes, consistent reads)&#10; * - Schema evolution (add columns without breaking queries)&#10; * - Time travel (query historical snapshots)&#10; * - Partition evolution (change partitioning without rewriting data)&#10; * - Hidden partitioning (partition pruning without WHERE clauses)&#10; *&#10; * PARTITIONING STRATEGY:&#10; * - All tables partitioned by load_date (daily ingestion pattern)&#10; * - Satellites also use valid_from for temporal queries&#10; * - Iceberg transforms: days(timestamp) for automatic bucketing&#10; * ========================================================================&#10; */&#10;&#10;import org.apache.spark.sql.SparkSession&#10;import org.apache.spark.sql.types._&#10;&#10;object RawVaultSchema {&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ CREATE ALL RAW VAULT TABLES                                     │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   */&#10;  def createAllTables()(implicit spark: SparkSession): Unit = {&#10;&#10;    println(&quot;&quot;&quot;&#10;         |╔════════════════════════════════════════════════════════════════╗&#10;         |║      CREATING DATA VAULT 2.0 RAW VAULT SCHEMA (BRONZE)        ║&#10;         |╚════════════════════════════════════════════════════════════════╝&#10;         |&quot;&quot;&quot;.stripMargin)&#10;&#10;    // Create database if not exists&#10;    println(&quot; Creating database 'bronze'...&quot;)&#10;    try {&#10;      // For HadoopCatalog, we need to specify a location&#10;      // For HiveCatalog, location is optional&#10;      spark.sql(&quot;&quot;&quot;&#10;        CREATE DATABASE IF NOT EXISTS bronze&#10;        COMMENT 'Data Vault 2.0 Raw Vault - Bronze Layer'&#10;      &quot;&quot;&quot;)&#10;&#10;      // Verify database was created by listing databases&#10;      val databases = spark.sql(&quot;SHOW DATABASES&quot;).collect().map(_.getString(0))&#10;      if (databases.contains(&quot;bronze&quot;)) {&#10;        println(&quot;✅ Database 'bronze' ready&quot;)&#10;      } else {&#10;        println(&quot;⚠️  WARNING: Database 'bronze' not found after creation!&quot;)&#10;        println(s&quot;   Available databases: ${databases.mkString(&quot;, &quot;)}&quot;)&#10;      }&#10;    } catch {&#10;      case e: Exception =&gt;&#10;        println(s&quot;❌ Failed to create database 'bronze': ${e.getMessage}&quot;)&#10;        throw e&#10;    }&#10;&#10;    // Create Hubs (Business entities - business keys only)&#10;    createHubCustomer()&#10;    createHubAccount()&#10;    createHubTransaction()&#10;    createHubTransactionItem()&#10;&#10;    // Create Links (Relationships between hubs)&#10;    createLinkCustomerAccount()&#10;    createLinkTransactionItem()&#10;&#10;    // Create Satellites (Descriptive attributes with history)&#10;    createSatCustomer()&#10;    createSatAccount()&#10;    createSatTransaction()&#10;    createSatTransactionItem()&#10;&#10;    // Create Load Metadata table (Audit trail)&#10;    createLoadMetadata()&#10;&#10;    println(&quot;\n✅ All Raw Vault tables created successfully&quot;)&#10;&#10;    // Verify tables were created&#10;    println(&quot;\n Verifying created tables...&quot;)&#10;    val tables = spark.sql(&quot;SHOW TABLES IN bronze&quot;).collect()&#10;    tables.foreach(row =&gt; println(s&quot;   - ${row.getString(1)}&quot;))&#10;&#10;    if (tables.isEmpty) {&#10;      println(&quot;⚠️  WARNING: No tables found in bronze database!&quot;)&#10;    }&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ HUB_CUSTOMER - Customer Business Keys                          │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   *&#10;   * DATA VAULT HUB PATTERN:&#10;   * - Stores only unique business keys (customer_id)&#10;   * - Hash key for deterministic joins&#10;   * - No descriptive attributes (those go in satellites)&#10;   * - Insert-only (immutable)&#10;   */&#10;  def createHubCustomer()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.hub_customer (&#10;        customer_hash_key STRING COMMENT 'MD5 hash of customer_id',&#10;        customer_id INT COMMENT 'Business key from source system',&#10;        load_date DATE COMMENT 'Date when record was loaded into vault',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Hub: Customer business keys only'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Hub_Customer&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ HUB_ACCOUNT - Account Business Keys                            │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   */&#10;  def createHubAccount()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.hub_account (&#10;        account_hash_key STRING COMMENT 'MD5 hash of account_id',&#10;        account_id INT COMMENT 'Business key from source system',&#10;        load_date DATE COMMENT 'Date when record was loaded into vault',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Hub: Account business keys only'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Hub_Account&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ HUB_TRANSACTION - Transaction Business Keys                    │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   */&#10;  def createHubTransaction()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.hub_transaction (&#10;        transaction_hash_key STRING COMMENT 'MD5 hash of transaction_id',&#10;        transaction_id INT COMMENT 'Business key from source system',&#10;        load_date DATE COMMENT 'Date when record was loaded into vault',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Hub: Transaction business keys only'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Hub_Transaction&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ HUB_TRANSACTION_ITEM - Transaction Item Business Keys          │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   *&#10;   * MULTI-ITEM PATTERN:&#10;   * - Composite business key: transaction_id + item_sequence&#10;   * - Enables modeling of transactions with multiple line items&#10;   * - Similar to e-commerce shopping cart pattern&#10;   */&#10;  def createHubTransactionItem()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.hub_transaction_item (&#10;        transaction_item_hash_key STRING COMMENT 'MD5 hash of transaction_id + item_sequence',&#10;        transaction_id INT COMMENT 'Parent transaction business key',&#10;        item_sequence INT COMMENT 'Item sequence number within transaction',&#10;        load_date DATE COMMENT 'Date when record was loaded into vault',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Hub: Transaction item business keys (multi-item pattern)'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Hub_Transaction_Item&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ LINK_CUSTOMER_ACCOUNT - Customer-Account Relationship          │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   *&#10;   * DATA VAULT LINK PATTERN:&#10;   * - Connects Hub_Customer to Hub_Account&#10;   * - Link hash key = MD5(customer_hash_key || account_hash_key)&#10;   * - No descriptive attributes&#10;   * - Captures many-to-many relationships&#10;   */&#10;  def createLinkCustomerAccount()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.link_customer_account (&#10;        link_customer_account_hash_key STRING COMMENT 'MD5 hash of customer_hash_key + account_hash_key',&#10;        customer_hash_key STRING COMMENT 'Foreign key to Hub_Customer',&#10;        account_hash_key STRING COMMENT 'Foreign key to Hub_Account',&#10;        load_date DATE COMMENT 'Date when relationship was established',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Link: Customer-Account many-to-many relationship'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Link_Customer_Account&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ LINK_TRANSACTION_ITEM - Transaction-Item Relationship          │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   */&#10;  def createLinkTransactionItem()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.link_transaction_item (&#10;        link_transaction_item_hash_key STRING COMMENT 'MD5 hash of transaction_hash_key + transaction_item_hash_key',&#10;        transaction_hash_key STRING COMMENT 'Foreign key to Hub_Transaction',&#10;        transaction_item_hash_key STRING COMMENT 'Foreign key to Hub_Transaction_Item',&#10;        load_date DATE COMMENT 'Date when relationship was established',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Link: Transaction-Item relationship (multi-item pattern)'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Link_Transaction_Item&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ SAT_CUSTOMER - Customer Descriptive Attributes                  │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   *&#10;   * DATA VAULT SATELLITE PATTERN:&#10;   * - Full history of attribute changes&#10;   * - Temporal tracking: valid_from, valid_to&#10;   * - Diff hash for change detection&#10;   * - Current records have valid_to = NULL&#10;   */&#10;  def createSatCustomer()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.sat_customer (&#10;        customer_hash_key STRING COMMENT 'Foreign key to Hub_Customer',&#10;        customer_type STRING COMMENT 'INDIVIDUAL or BUSINESS',&#10;        first_name STRING COMMENT 'First name (for individuals)',&#10;        last_name STRING COMMENT 'Last name (for individuals)',&#10;        company_name STRING COMMENT 'Company name (for business)',&#10;        email STRING COMMENT 'Email address',&#10;        phone STRING COMMENT 'Phone number',&#10;        date_of_birth DATE COMMENT 'Date of birth (for individuals)',&#10;        tax_id STRING COMMENT 'Tax identification number',&#10;        customer_since DATE COMMENT 'Date customer relationship started',&#10;        customer_status STRING COMMENT 'ACTIVE, INACTIVE, SUSPENDED',&#10;        credit_score INT COMMENT 'Credit score (if available)',&#10;        customer_diff_hash STRING COMMENT 'MD5 hash of all descriptive columns for change detection',&#10;        valid_from TIMESTAMP COMMENT 'Start of validity period for this version',&#10;        valid_to TIMESTAMP COMMENT 'End of validity period (NULL for current version)',&#10;        load_date DATE COMMENT 'Date when record was loaded into vault',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Satellite: Customer descriptive attributes with full history (SCD Type 2)'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;  ️  Created Sat_Customer&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ SAT_ACCOUNT - Account Descriptive Attributes                    │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   */&#10;  def createSatAccount()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.sat_account (&#10;        account_hash_key STRING COMMENT 'Foreign key to Hub_Account',&#10;        account_number STRING COMMENT 'Account number (display)',&#10;        account_type STRING COMMENT 'CHECKING, SAVINGS, CREDIT, LOAN',&#10;        product_id INT COMMENT 'Product identifier',&#10;        branch_id INT COMMENT 'Branch where account was opened',&#10;        balance DECIMAL(15,2) COMMENT 'Current balance',&#10;        available_balance DECIMAL(15,2) COMMENT 'Available balance (balance - holds)',&#10;        currency_code STRING COMMENT 'Currency code (USD, EUR, etc.)',&#10;        interest_rate DECIMAL(5,4) COMMENT 'Annual interest rate',&#10;        credit_limit DECIMAL(15,2) COMMENT 'Credit limit (for credit accounts)',&#10;        opened_date DATE COMMENT 'Date account was opened',&#10;        account_status STRING COMMENT 'ACTIVE, CLOSED, FROZEN',&#10;        account_diff_hash STRING COMMENT 'MD5 hash of all descriptive columns',&#10;        valid_from TIMESTAMP COMMENT 'Start of validity period',&#10;        valid_to TIMESTAMP COMMENT 'End of validity period (NULL for current)',&#10;        load_date DATE COMMENT 'Date when record was loaded',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Satellite: Account descriptive attributes with history'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;  ️  Created Sat_Account&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ SAT_TRANSACTION - Transaction Descriptive Attributes           │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   */&#10;  def createSatTransaction()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.sat_transaction (&#10;        transaction_hash_key STRING COMMENT 'Foreign key to Hub_Transaction',&#10;        transaction_number STRING COMMENT 'Transaction number (display)',&#10;        transaction_type STRING COMMENT 'DEPOSIT, WITHDRAWAL, TRANSFER, PAYMENT',&#10;        transaction_date DATE COMMENT 'Date of transaction',&#10;        transaction_time TIMESTAMP COMMENT 'Timestamp of transaction',&#10;        total_amount DECIMAL(15,2) COMMENT 'Total transaction amount',&#10;        currency_code STRING COMMENT 'Currency code',&#10;        description STRING COMMENT 'Transaction description',&#10;        channel STRING COMMENT 'BRANCH, ATM, ONLINE, MOBILE',&#10;        status STRING COMMENT 'PENDING, COMPLETED, FAILED, REVERSED',&#10;        transaction_diff_hash STRING COMMENT 'MD5 hash of all descriptive columns',&#10;        valid_from TIMESTAMP COMMENT 'Start of validity period',&#10;        valid_to TIMESTAMP COMMENT 'End of validity period (NULL for current)',&#10;        load_date DATE COMMENT 'Date when record was loaded',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Satellite: Transaction header attributes with history'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;  ️  Created Sat_Transaction&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ SAT_TRANSACTION_ITEM - Transaction Item Attributes             │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   *&#10;   * MULTI-ITEM PATTERN:&#10;   * - Each transaction can have multiple line items&#10;   * - Sum of item_amount should equal transaction total_amount&#10;   * - Supports complex transaction scenarios (e.g., split payments)&#10;   */&#10;  def createSatTransactionItem()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.sat_transaction_item (&#10;        transaction_item_hash_key STRING COMMENT 'Foreign key to Hub_Transaction_Item',&#10;        item_type STRING COMMENT 'DEBIT, CREDIT, FEE, TAX',&#10;        item_description STRING COMMENT 'Item description',&#10;        item_amount DECIMAL(15,2) COMMENT 'Item amount',&#10;        item_category STRING COMMENT 'Category code',&#10;        merchant_id INT COMMENT 'Merchant identifier (if applicable)',&#10;        item_diff_hash STRING COMMENT 'MD5 hash of all descriptive columns',&#10;        valid_from TIMESTAMP COMMENT 'Start of validity period',&#10;        valid_to TIMESTAMP COMMENT 'End of validity period (NULL for current)',&#10;        load_date DATE COMMENT 'Date when record was loaded',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Satellite: Transaction item line details (multi-item pattern)'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;  ️  Created Sat_Transaction_Item&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ LOAD_METADATA - ETL Audit Trail                                │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   *&#10;   * PURPOSE:&#10;   * Track all ETL load operations for auditing and troubleshooting.&#10;   * Essential for data lineage and compliance.&#10;   */&#10;  def createLoadMetadata()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.load_metadata (&#10;        load_id BIGINT COMMENT 'Unique load identifier',&#10;        entity_name STRING COMMENT 'Entity being loaded (customer, account, etc.)',&#10;        record_source STRING COMMENT 'Source system',&#10;        load_date DATE COMMENT 'Date of load',&#10;        load_start_timestamp TIMESTAMP COMMENT 'When load started',&#10;        load_end_timestamp TIMESTAMP COMMENT 'When load completed',&#10;        load_status STRING COMMENT 'SUCCESS, FAILED, IN_PROGRESS',&#10;        records_extracted BIGINT COMMENT 'Number of records read from source',&#10;        records_loaded BIGINT COMMENT 'Number of records written to vault',&#10;        error_message STRING COMMENT 'Error message if failed',&#10;        load_duration_seconds INT COMMENT 'Duration in seconds'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'ETL load metadata for audit trail and monitoring'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Load_Metadata&quot;)&#10;  }&#10;}&#10;&#10;" />
              <option name="updatedContent" value="package bronze&#10;&#10;/**&#10; * ========================================================================&#10; * RAW VAULT SCHEMA - DATA VAULT 2.0 TABLE DEFINITIONS&#10; * ========================================================================&#10; *&#10; * PURPOSE:&#10; * Defines and creates all Data Vault 2.0 tables in the Bronze layer&#10; * (Raw Vault) using Apache Iceberg table format.&#10; *&#10; * DATA VAULT 2.0 ENTITIES:&#10; * - Hubs: Business entities (Customer, Account, Transaction, Transaction Item)&#10; * - Links: Relationships between hubs (Customer-Account, Transaction-Item)&#10; * - Satellites: Descriptive attributes with full history tracking&#10; *&#10; * ICEBERG TABLE BENEFITS:&#10; * - ACID transactions (atomic writes, consistent reads)&#10; * - Schema evolution (add columns without breaking queries)&#10; * - Time travel (query historical snapshots)&#10; * - Partition evolution (change partitioning without rewriting data)&#10; * - Hidden partitioning (partition pruning without WHERE clauses)&#10; *&#10; * PARTITIONING STRATEGY:&#10; * - All tables partitioned by load_date (daily ingestion pattern)&#10; * - Satellites also use valid_from for temporal queries&#10; * - Iceberg transforms: days(timestamp) for automatic bucketing&#10; * ========================================================================&#10; */&#10;&#10;import org.apache.spark.sql.SparkSession&#10;import org.apache.spark.sql.types._&#10;&#10;object RawVaultSchema {&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ CREATE ALL RAW VAULT TABLES                                     │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   */&#10;  def createAllTables()(implicit spark: SparkSession): Unit = {&#10;&#10;    println(&quot;&quot;&quot;&#10;         |╔════════════════════════════════════════════════════════════════╗&#10;         |║      CREATING DATA VAULT 2.0 RAW VAULT SCHEMA (BRONZE)        ║&#10;         |╚════════════════════════════════════════════════════════════════╝&#10;         |&quot;&quot;&quot;.stripMargin)&#10;&#10;    // Create database if not exists&#10;    println(&quot; Creating database 'bronze'...&quot;)&#10;    try {&#10;      // For HadoopCatalog, we need to specify a location&#10;      // For HiveCatalog, location is optional&#10;      spark.sql(&quot;&quot;&quot;&#10;        CREATE DATABASE IF NOT EXISTS bronze&#10;        COMMENT 'Data Vault 2.0 Raw Vault - Bronze Layer'&#10;      &quot;&quot;&quot;)&#10;&#10;      // Verify database was created by listing databases&#10;      val databases = spark.sql(&quot;SHOW DATABASES&quot;).collect().map(_.getString(0))&#10;      if (databases.contains(&quot;bronze&quot;)) {&#10;        println(&quot;✅ Database 'bronze' ready&quot;)&#10;      } else {&#10;        println(&quot;⚠️  WARNING: Database 'bronze' not found after creation!&quot;)&#10;        println(s&quot;   Available databases: ${databases.mkString(&quot;, &quot;)}&quot;)&#10;      }&#10;    } catch {&#10;      case e: Exception =&gt;&#10;        println(s&quot;❌ Failed to create database 'bronze': ${e.getMessage}&quot;)&#10;        throw e&#10;    }&#10;&#10;    // Create Hubs (Business entities - business keys only)&#10;    createHubCustomer()&#10;    createHubAccount()&#10;    createHubTransaction()&#10;    createHubTransactionItem()&#10;&#10;    // Create Links (Relationships between hubs)&#10;    createLinkCustomerAccount()&#10;    createLinkTransactionItem()&#10;&#10;    // Create Satellites (Descriptive attributes with history)&#10;    createSatCustomer()&#10;    createSatAccount()&#10;    createSatTransaction()&#10;    createSatTransactionItem()&#10;&#10;    // Create Load Metadata table (Audit trail)&#10;    createLoadMetadata()&#10;&#10;    println(&quot;\n✅ All Raw Vault tables created successfully&quot;)&#10;&#10;    // Verify tables were created&#10;    println(&quot;\n Verifying created tables...&quot;)&#10;    val tables = spark.sql(&quot;SHOW TABLES IN bronze&quot;).collect()&#10;    tables.foreach(row =&gt; println(s&quot;   - ${row.getString(1)}&quot;))&#10;&#10;    if (tables.isEmpty) {&#10;      println(&quot;⚠️  WARNING: No tables found in bronze database!&quot;)&#10;    }&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ HUB_CUSTOMER - Customer Business Keys                          │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   *&#10;   * DATA VAULT HUB PATTERN:&#10;   * - Stores only unique business keys (customer_id)&#10;   * - Hash key for deterministic joins&#10;   * - No descriptive attributes (those go in satellites)&#10;   * - Insert-only (immutable)&#10;   */&#10;  def createHubCustomer()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.hub_customer (&#10;        customer_hash_key STRING COMMENT 'MD5 hash of customer_id',&#10;        customer_id INT COMMENT 'Business key from source system',&#10;        load_date DATE COMMENT 'Date when record was loaded into vault',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Hub: Customer business keys only'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Hub_Customer&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ HUB_ACCOUNT - Account Business Keys                            │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   */&#10;  def createHubAccount()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.hub_account (&#10;        account_hash_key STRING COMMENT 'MD5 hash of account_id',&#10;        account_id INT COMMENT 'Business key from source system',&#10;        load_date DATE COMMENT 'Date when record was loaded into vault',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Hub: Account business keys only'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Hub_Account&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ HUB_TRANSACTION - Transaction Business Keys                    │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   */&#10;  def createHubTransaction()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.hub_transaction (&#10;        transaction_hash_key STRING COMMENT 'MD5 hash of transaction_id',&#10;        transaction_id INT COMMENT 'Business key from source system',&#10;        load_date DATE COMMENT 'Date when record was loaded into vault',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Hub: Transaction business keys only'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Hub_Transaction&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ HUB_TRANSACTION_ITEM - Transaction Item Business Keys          │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   *&#10;   * MULTI-ITEM PATTERN:&#10;   * - Composite business key: transaction_id + item_sequence&#10;   * - Enables modeling of transactions with multiple line items&#10;   * - Similar to e-commerce shopping cart pattern&#10;   */&#10;  def createHubTransactionItem()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.hub_transaction_item (&#10;        transaction_item_hash_key STRING COMMENT 'MD5 hash of transaction_id + item_sequence',&#10;        transaction_id INT COMMENT 'Parent transaction business key',&#10;        item_sequence INT COMMENT 'Item sequence number within transaction',&#10;        load_date DATE COMMENT 'Date when record was loaded into vault',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Hub: Transaction item business keys (multi-item pattern)'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Hub_Transaction_Item&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ LINK_CUSTOMER_ACCOUNT - Customer-Account Relationship          │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   *&#10;   * DATA VAULT LINK PATTERN:&#10;   * - Connects Hub_Customer to Hub_Account&#10;   * - Link hash key = MD5(customer_hash_key || account_hash_key)&#10;   * - No descriptive attributes&#10;   * - Captures many-to-many relationships&#10;   */&#10;  def createLinkCustomerAccount()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.link_customer_account (&#10;        link_customer_account_hash_key STRING COMMENT 'MD5 hash of customer_hash_key + account_hash_key',&#10;        customer_hash_key STRING COMMENT 'Foreign key to Hub_Customer',&#10;        account_hash_key STRING COMMENT 'Foreign key to Hub_Account',&#10;        load_date DATE COMMENT 'Date when relationship was established',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Link: Customer-Account many-to-many relationship'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Link_Customer_Account&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ LINK_TRANSACTION_ITEM - Transaction-Item Relationship          │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   */&#10;  def createLinkTransactionItem()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.link_transaction_item (&#10;        link_transaction_item_hash_key STRING COMMENT 'MD5 hash of transaction_hash_key + transaction_item_hash_key',&#10;        transaction_hash_key STRING COMMENT 'Foreign key to Hub_Transaction',&#10;        transaction_item_hash_key STRING COMMENT 'Foreign key to Hub_Transaction_Item',&#10;        load_date DATE COMMENT 'Date when relationship was established',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Link: Transaction-Item relationship (multi-item pattern)'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Link_Transaction_Item&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ SAT_CUSTOMER - Customer Descriptive Attributes                  │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   *&#10;   * DATA VAULT SATELLITE PATTERN:&#10;   * - Full history of attribute changes&#10;   * - Temporal tracking: valid_from, valid_to&#10;   * - Diff hash for change detection&#10;   * - Current records have valid_to = NULL&#10;   */&#10;  def createSatCustomer()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.sat_customer (&#10;        customer_hash_key STRING COMMENT 'Foreign key to Hub_Customer',&#10;        customer_type STRING COMMENT 'INDIVIDUAL or BUSINESS',&#10;        first_name STRING COMMENT 'First name (for individuals)',&#10;        last_name STRING COMMENT 'Last name (for individuals)',&#10;        business_name STRING COMMENT 'Business name (for business customers)',&#10;        email STRING COMMENT 'Email address',&#10;        phone STRING COMMENT 'Phone number',&#10;        date_of_birth DATE COMMENT 'Date of birth (for individuals)',&#10;        ssn STRING COMMENT 'Social Security Number (for individuals)',&#10;        tax_id STRING COMMENT 'Tax identification number (for business customers)',&#10;        credit_score INT COMMENT 'Credit score (if available)',&#10;        customer_since DATE COMMENT 'Date customer relationship started',&#10;        loyalty_tier STRING COMMENT 'Loyalty tier from source (STANDARD, SILVER, GOLD, PLATINUM)',&#10;        preferred_contact_method STRING COMMENT 'Preferred contact method from source (EMAIL, PHONE, SMS, MAIL)',&#10;        customer_diff_hash STRING COMMENT 'MD5 hash of descriptive columns for change detection',&#10;        valid_from TIMESTAMP COMMENT 'Start of validity period for this version',&#10;        valid_to TIMESTAMP COMMENT 'End of validity period (NULL for current version)',&#10;        load_date DATE COMMENT 'Date when record was loaded into vault',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Satellite: Customer descriptive attributes (raw) with full history (SCD Type 2)'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;  ️  Created Sat_Customer&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ SAT_ACCOUNT - Account Descriptive Attributes                    │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   */&#10;  def createSatAccount()(implicit spark: SparkSession): Unit = {&#10;    // Account satellite aligned with account.avsc (raw attributes only)&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.sat_account (&#10;        account_hash_key STRING COMMENT 'Foreign key to Hub_Account',&#10;        account_number STRING COMMENT 'Customer-facing account number',&#10;        product_id INT COMMENT 'Product identifier',&#10;        branch_id INT COMMENT 'Branch where account was opened',&#10;        account_status STRING COMMENT 'ACTIVE, CLOSED, FROZEN, SUSPENDED',&#10;        current_balance DECIMAL(15,2) COMMENT 'Current account balance',&#10;        available_balance DECIMAL(15,2) COMMENT 'Available balance',&#10;        currency STRING COMMENT 'ISO 4217 currency code',&#10;        overdraft_limit DECIMAL(15,2) COMMENT 'Overdraft protection limit',&#10;        interest_rate DECIMAL(5,2) COMMENT 'Annual interest rate',&#10;        opened_date DATE COMMENT 'Date account was opened',&#10;        closed_date DATE COMMENT 'Date account was closed (NULL for active)',&#10;        last_transaction_date TIMESTAMP COMMENT 'Timestamp of most recent transaction',&#10;        updated_at TIMESTAMP COMMENT 'CDC last modification timestamp',&#10;        account_diff_hash STRING COMMENT 'MD5 hash of descriptive columns for change detection',&#10;        valid_from TIMESTAMP COMMENT 'Start of validity period',&#10;        valid_to TIMESTAMP COMMENT 'End of validity period (NULL for current)',&#10;        load_date DATE COMMENT 'Date when record was loaded',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Satellite: Account raw descriptive attributes with history'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;  ️  Created Sat_Account&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ SAT_TRANSACTION - Transaction Descriptive Attributes           │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   */&#10;  def createSatTransaction()(implicit spark: SparkSession): Unit = {&#10;    // Transaction header satellite aligned with transaction_header.avsc (raw attributes only)&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.sat_transaction (&#10;        transaction_hash_key STRING COMMENT 'Foreign key to Hub_Transaction',&#10;        transaction_number STRING COMMENT 'User-facing transaction reference',&#10;        transaction_type STRING COMMENT 'High-level transaction category',&#10;        transaction_date TIMESTAMP COMMENT 'When transaction occurred',&#10;        posting_date TIMESTAMP COMMENT 'When transaction was posted',&#10;        total_amount DECIMAL(15,2) COMMENT 'Total transaction amount',&#10;        description STRING COMMENT 'Transaction description from source',&#10;        channel STRING COMMENT 'Channel (ATM, BRANCH, ONLINE, MOBILE, PHONE)',&#10;        transaction_status STRING COMMENT 'PENDING, COMPLETED, FAILED, REVERSED',&#10;        location STRING COMMENT 'Where transaction occurred',&#10;        reference_number STRING COMMENT 'External reference number',&#10;        initiated_by STRING COMMENT 'User or system that initiated transaction',&#10;        created_at TIMESTAMP COMMENT 'Creation timestamp in source',&#10;        updated_at TIMESTAMP COMMENT 'CDC last modification timestamp',&#10;        transaction_diff_hash STRING COMMENT 'MD5 hash of descriptive columns',&#10;        valid_from TIMESTAMP COMMENT 'Start of validity period',&#10;        valid_to TIMESTAMP COMMENT 'End of validity period (NULL for current)',&#10;        load_date DATE COMMENT 'Date when record was loaded',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Satellite: Transaction header raw attributes with history'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;  ️  Created Sat_Transaction&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ SAT_TRANSACTION_ITEM - Transaction Item Attributes             │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   *&#10;   * MULTI-ITEM PATTERN:&#10;   * - Each transaction can have multiple line items&#10;   * - Sum of item_amount should equal transaction total_amount&#10;   * - Supports complex transaction scenarios (e.g., split payments)&#10;   */&#10;  def createSatTransactionItem()(implicit spark: SparkSession): Unit = {&#10;    // Transaction item satellite aligned with transaction_item.avsc (raw attributes only)&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.sat_transaction_item (&#10;        transaction_item_hash_key STRING COMMENT 'Foreign key to Hub_Transaction_Item',&#10;        item_amount DECIMAL(15,2) COMMENT 'Line item amount',&#10;        category_id INT COMMENT 'Category reference identifier',&#10;        merchant_name STRING COMMENT 'Merchant or payee name',&#10;        merchant_category_code STRING COMMENT 'Merchant Category Code (MCC)',&#10;        item_description STRING COMMENT 'Detailed item description',&#10;        created_at TIMESTAMP COMMENT 'Item creation timestamp in source',&#10;        payee_name STRING COMMENT 'Payment recipient name',&#10;        payee_account STRING COMMENT 'Payee account number',&#10;        is_recurring BOOLEAN COMMENT 'Is this a recurring payment item',&#10;        item_diff_hash STRING COMMENT 'MD5 hash of descriptive columns',&#10;        valid_from TIMESTAMP COMMENT 'Start of validity period',&#10;        valid_to TIMESTAMP COMMENT 'End of validity period (NULL for current)',&#10;        load_date DATE COMMENT 'Date when record was loaded',&#10;        record_source STRING COMMENT 'Source system identifier'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'Data Vault Satellite: Transaction item raw attributes (multi-item pattern) with history'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;  ️  Created Sat_Transaction_Item&quot;)&#10;  }&#10;&#10;  /**&#10;   * ┌─────────────────────────────────────────────────────────────────┐&#10;   * │ LOAD_METADATA - ETL Audit Trail                                │&#10;   * └─────────────────────────────────────────────────────────────────┘&#10;   *&#10;   * PURPOSE:&#10;   * Track all ETL load operations for auditing and troubleshooting.&#10;   * Essential for data lineage and compliance.&#10;   */&#10;  def createLoadMetadata()(implicit spark: SparkSession): Unit = {&#10;    spark.sql(&quot;&quot;&quot;&#10;      CREATE TABLE IF NOT EXISTS bronze.load_metadata (&#10;        load_id BIGINT COMMENT 'Unique load identifier',&#10;        entity_name STRING COMMENT 'Entity being loaded (customer, account, etc.)',&#10;        record_source STRING COMMENT 'Source system',&#10;        load_date DATE COMMENT 'Date of load',&#10;        load_start_timestamp TIMESTAMP COMMENT 'When load started',&#10;        load_end_timestamp TIMESTAMP COMMENT 'When load completed',&#10;        load_status STRING COMMENT 'SUCCESS, FAILED, IN_PROGRESS',&#10;        records_extracted BIGINT COMMENT 'Number of records read from source',&#10;        records_loaded BIGINT COMMENT 'Number of records written to vault',&#10;        error_message STRING COMMENT 'Error message if failed',&#10;        load_duration_seconds INT COMMENT 'Duration in seconds'&#10;      )&#10;      USING iceberg&#10;      PARTITIONED BY (load_date)&#10;      TBLPROPERTIES (&#10;        'format-version' = '2',&#10;        'write.parquet.compression-codec' = 'snappy',&#10;        'comment' = 'ETL load metadata for audit trail and monitoring'&#10;      )&#10;    &quot;&quot;&quot;)&#10;    println(&quot;   Created Load_Metadata&quot;)&#10;  }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>