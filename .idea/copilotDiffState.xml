<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/.jvmopts">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/.jvmopts" />
              <option name="updatedContent" value="-Xms512m&#10;-Xmx2g&#10;-Dfile.encoding=UTF-8&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/.sbtopts">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/.sbtopts" />
              <option name="updatedContent" value="-Dsbt.server.forcestart=false&#10;-Dsbt.server=false&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/ASSEMBLY_GUIDE.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/ASSEMBLY_GUIDE.md" />
              <option name="updatedContent" value="# Assembly &amp; Deployment Guide&#10;&#10;## Overview&#10;&#10;This project uses an **uber JAR strategy** that balances portability with JAR size efficiency:&#10;&#10;- ✅ **Included in JAR**: Iceberg extensions, Hive Metastore client, JDBC drivers, utility libraries&#10;- ❌ **Excluded from JAR**: Spark core/SQL/Hive, Hadoop, Scala library (provided by SPARK_HOME)&#10;&#10;**Result**: A ~30-40 MB portable JAR that works identically in local and cluster environments.&#10;&#10;---&#10;&#10;## Building the Uber JAR&#10;&#10;### Quick Build&#10;&#10;```bash&#10;# Clean and build the uber JAR&#10;sbt clean assembly&#10;```&#10;&#10;**Output**: `target/scala-2.12/data-vault-modeling-etl-1.0.0.jar`&#10;&#10;### What Gets Included&#10;&#10;The assembled JAR contains:&#10;&#10;| Component | Size | Reason |&#10;|-----------|------|--------|&#10;| **Iceberg Runtime** | ~8 MB | Not available in standard Spark distributions |&#10;| **Iceberg Core** | ~2 MB | Required for table format operations |&#10;| **Iceberg HMS** | ~1 MB | Hive Metastore catalog integration |&#10;| **Hive Metastore Client** | ~5 MB | Required for schema management |&#10;| **PostgreSQL JDBC** | ~1 MB | Source database connectivity |&#10;| **Derby Embedded DB** | ~3 MB | Local HMS for development |&#10;| **Avro Libraries** | ~2 MB | Schema handling for NiFi CDC |&#10;| **Jackson 2.15.2** | ~2 MB | Forced version for compatibility |&#10;| **Utility Libraries** | ~5 MB | Config, commons-lang3, joda-time |&#10;| **Your Application Code** | ~1 MB | ETL jobs and utilities |&#10;| **Total** | ~30 MB | Highly portable, reasonably sized |&#10;&#10;### What Gets Excluded&#10;&#10;These are marked as `provided` and come from SPARK_HOME or the cluster:&#10;&#10;- Spark Core, SQL, Hive, Avro (~150 MB)&#10;- Hadoop Client, Common (~50 MB)&#10;- Scala Library (~10 MB)&#10;&#10;---&#10;&#10;## Running Locally&#10;&#10;### Prerequisites&#10;&#10;Ensure `SPARK_HOME` is set and points to a Spark 3.5.x installation:&#10;&#10;```bash&#10;# Windows PowerShell&#10;$env:SPARK_HOME = &quot;C:\spark-3.5.0-bin-hadoop3&quot;&#10;$env:PATH += &quot;;$env:SPARK_HOME\bin&quot;&#10;&#10;# Verify&#10;spark-submit --version&#10;```&#10;&#10;### Configuration&#10;&#10;Edit `src/main/resources/application.properties`:&#10;&#10;```properties&#10;# Warehouse &amp; Catalog&#10;spark.sql.catalog.spark_catalog.warehouse=warehouse&#10;# spark.sql.catalog.spark_catalog.uri=thrift://localhost:9083  # Uncomment for external HMS&#10;&#10;# Networking (important for Windows local execution)&#10;spark.driver.host=127.0.0.1&#10;spark.driver.bindAddress=0.0.0.0&#10;&#10;# Data Paths&#10;staging.path=warehouse/staging&#10;bronze.path=warehouse/bronze&#10;silver.path=warehouse/silver&#10;gold.path=warehouse/gold&#10;&#10;# ETL Defaults&#10;etl.mode=incremental&#10;etl.record.source=PostgreSQL&#10;```&#10;&#10;### Execute ETL Jobs&#10;&#10;```bash&#10;# Bronze layer - full load&#10;spark-submit \&#10;  --class bronze.RawVaultETL \&#10;  --master local[*] \&#10;  target/scala-2.12/data-vault-modeling-etl-1.0.0.jar \&#10;  --mode full&#10;&#10;# Bronze layer - incremental load&#10;spark-submit \&#10;  --class bronze.RawVaultETL \&#10;  --master local[*] \&#10;  target/scala-2.12/data-vault-modeling-etl-1.0.0.jar \&#10;  --mode incremental --entity customer&#10;&#10;# Silver layer - build PIT tables&#10;spark-submit \&#10;  --class silver.BusinessVaultETL \&#10;  --master local[*] \&#10;  target/scala-2.12/data-vault-modeling-etl-1.0.0.jar \&#10;  --build-pit&#10;&#10;# Gold layer - load dimensions&#10;spark-submit \&#10;  --class gold.DimensionalModelETL \&#10;  --master local[*] \&#10;  target/scala-2.12/data-vault-modeling-etl-1.0.0.jar \&#10;  --load-dimensions&#10;```&#10;&#10;### Windows PowerShell Helper Script&#10;&#10;```powershell&#10;# run-etl.ps1&#10;param(&#10;    [string]$Layer = &quot;bronze&quot;,&#10;    [string]$Class = &quot;RawVaultETL&quot;,&#10;    [string]$Mode = &quot;incremental&quot;,&#10;    [string]$Entity = &quot;&quot;&#10;)&#10;&#10;$jarPath = &quot;target\scala-2.12\data-vault-modeling-etl-1.0.0.jar&quot;&#10;$fullClass = &quot;$Layer.$Class&quot;&#10;$args = &quot;--mode $Mode&quot;&#10;&#10;if ($Entity) {&#10;    $args += &quot; --entity $Entity&quot;&#10;}&#10;&#10;Write-Host &quot; Running $fullClass with args: $args&quot; -ForegroundColor Cyan&#10;&#10;spark-submit `&#10;    --class $fullClass `&#10;    --master &quot;local[*]&quot; `&#10;    $jarPath `&#10;    $args.Split(&quot; &quot;)&#10;&#10;if ($LASTEXITCODE -eq 0) {&#10;    Write-Host &quot;✅ ETL job completed successfully&quot; -ForegroundColor Green&#10;} else {&#10;    Write-Host &quot;❌ ETL job failed with exit code $LASTEXITCODE&quot; -ForegroundColor Red&#10;}&#10;```&#10;&#10;**Usage**:&#10;```powershell&#10;.\run-etl.ps1 -Layer bronze -Class RawVaultETL -Mode full&#10;.\run-etl.ps1 -Layer bronze -Class RawVaultETL -Mode incremental -Entity customer&#10;.\run-etl.ps1 -Layer silver -Class BusinessVaultETL&#10;```&#10;&#10;---&#10;&#10;## Running in Cluster (YARN/Kubernetes)&#10;&#10;### Cluster Deployment&#10;&#10;```bash&#10;# Copy JAR to edge node or cluster-accessible location&#10;scp target/scala-2.12/data-vault-modeling-etl-1.0.0.jar user@edge-node:/opt/etl/&#10;&#10;# SSH to edge node&#10;ssh user@edge-node&#10;```&#10;&#10;### YARN Cluster Mode&#10;&#10;```bash&#10;spark-submit \&#10;  --class bronze.RawVaultETL \&#10;  --master yarn \&#10;  --deploy-mode cluster \&#10;  --num-executors 10 \&#10;  --executor-cores 4 \&#10;  --executor-memory 8G \&#10;  --driver-memory 4G \&#10;  --conf spark.sql.catalog.spark_catalog.warehouse=hdfs://namenode:9000/warehouse \&#10;  --conf spark.sql.catalog.spark_catalog.uri=thrift://hive-metastore:9083 \&#10;  /opt/etl/data-vault-modeling-etl-1.0.0.jar \&#10;  --mode incremental&#10;```&#10;&#10;### YARN Client Mode (for debugging)&#10;&#10;```bash&#10;spark-submit \&#10;  --class bronze.RawVaultETL \&#10;  --master yarn \&#10;  --deploy-mode client \&#10;  --num-executors 5 \&#10;  --executor-cores 2 \&#10;  --executor-memory 4G \&#10;  /opt/etl/data-vault-modeling-etl-1.0.0.jar \&#10;  --mode full&#10;```&#10;&#10;### Kubernetes&#10;&#10;```bash&#10;spark-submit \&#10;  --class bronze.RawVaultETL \&#10;  --master k8s://https://kubernetes-api:6443 \&#10;  --deploy-mode cluster \&#10;  --conf spark.executor.instances=5 \&#10;  --conf spark.kubernetes.container.image=spark:3.5.0-scala2.12 \&#10;  --conf spark.kubernetes.file.upload.path=s3a://my-bucket/spark-jars \&#10;  local:///opt/spark/work-dir/data-vault-modeling-etl-1.0.0.jar \&#10;  --mode incremental&#10;```&#10;&#10;---&#10;&#10;## Configuration Override&#10;&#10;### Priority Order (Highest to Lowest)&#10;&#10;1. **JVM System Properties** (`-Dkey=value`)&#10;2. **Environment Variables** (`SPARK_WAREHOUSE`, `HIVE_METASTORE_URI`)&#10;3. **`application.properties` file** (bundled in JAR)&#10;4. **Hardcoded defaults** (in `ConfigLoader`)&#10;&#10;### Examples&#10;&#10;#### Override via spark-submit&#10;&#10;```bash&#10;spark-submit \&#10;  --class bronze.RawVaultETL \&#10;  --master local[*] \&#10;  --conf spark.sql.catalog.spark_catalog.warehouse=hdfs://namenode/warehouse \&#10;  --conf spark.sql.catalog.spark_catalog.uri=thrift://hive-metastore:9083 \&#10;  target/scala-2.12/data-vault-modeling-etl-1.0.0.jar \&#10;  --mode full&#10;```&#10;&#10;#### Override via Environment Variables&#10;&#10;```bash&#10;# PowerShell&#10;$env:SPARK_WAREHOUSE = &quot;hdfs://namenode/warehouse&quot;&#10;$env:HIVE_METASTORE_URI = &quot;thrift://hive-metastore:9083&quot;&#10;spark-submit --class bronze.RawVaultETL --master local[*] target/scala-2.12/data-vault-modeling-etl-1.0.0.jar&#10;&#10;# Bash&#10;export SPARK_WAREHOUSE=hdfs://namenode/warehouse&#10;export HIVE_METASTORE_URI=thrift://hive-metastore:9083&#10;spark-submit --class bronze.RawVaultETL --master local[*] target/scala-2.12/data-vault-modeling-etl-1.0.0.jar&#10;```&#10;&#10;#### Override via JVM Properties&#10;&#10;```bash&#10;spark-submit \&#10;  --class bronze.RawVaultETL \&#10;  --master local[*] \&#10;  --driver-java-options &quot;-Dwarehouse=custom_warehouse -Detl.mode=full&quot; \&#10;  target/scala-2.12/data-vault-modeling-etl-1.0.0.jar&#10;```&#10;&#10;---&#10;&#10;## Troubleshooting&#10;&#10;### ClassNotFoundException: IcebergSparkSessionExtensions&#10;&#10;**Problem**: Iceberg classes not found at runtime&#10;&#10;**Solution**: This should NOT happen with the uber JAR. Verify:&#10;&#10;```bash&#10;# Check if Iceberg is in the JAR&#10;jar tf target/scala-2.12/data-vault-modeling-etl-1.0.0.jar | grep iceberg&#10;&#10;# Should see:&#10;# org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.class&#10;# ... many other iceberg classes ...&#10;```&#10;&#10;If Iceberg is missing, rebuild:&#10;```bash&#10;sbt clean assembly&#10;```&#10;&#10;### Spark Version Mismatch&#10;&#10;**Problem**: Spark runtime version differs from compiled version&#10;&#10;**Symptom**: `NoSuchMethodError`, `IncompatibleClassChangeError`&#10;&#10;**Solution**: Ensure `SPARK_HOME` points to Spark 3.5.x:&#10;```bash&#10;spark-submit --version&#10;# Should show: version 3.5.x&#10;```&#10;&#10;### Hive Metastore Connection Issues&#10;&#10;**Problem**: Cannot connect to Hive Metastore&#10;&#10;**Symptoms**:&#10;- `TTransportException: Could not connect to localhost:9083`&#10;- `MetaException: Unable to connect to database`&#10;&#10;**Solutions**:&#10;&#10;1. **Local Development (Embedded Derby)**: Remove `spark.sql.catalog.spark_catalog.uri` from config&#10;2. **External HMS**: Ensure HMS is running and accessible:&#10;   ```bash&#10;   telnet hive-metastore 9083&#10;   ```&#10;3. **Check Firewall**: Ensure port 9083 is open&#10;&#10;### Out of Memory Errors&#10;&#10;**Problem**: `java.lang.OutOfMemoryError: Java heap space`&#10;&#10;**Solution**: Increase executor/driver memory:&#10;```bash&#10;spark-submit \&#10;  --class bronze.RawVaultETL \&#10;  --master local[*] \&#10;  --driver-memory 8G \&#10;  --executor-memory 8G \&#10;  target/scala-2.12/data-vault-modeling-etl-1.0.0.jar&#10;```&#10;&#10;---&#10;&#10;## Advanced: Build Profiles&#10;&#10;### Current Strategy (Recommended)&#10;&#10;One uber JAR works everywhere - no need for separate profiles.&#10;&#10;### Alternative: Slim JAR for Cluster&#10;&#10;If your cluster already has Iceberg installed, you can create a slim JAR by excluding Iceberg:&#10;&#10;**Modify `build.sbt`**:&#10;```scala&#10;// Mark Iceberg as provided&#10;&quot;org.apache.iceberg&quot; %% &quot;iceberg-spark-runtime-3.5&quot; % icebergVersion % &quot;provided&quot;&#10;```&#10;&#10;**Then use `--packages` in spark-submit**:&#10;```bash&#10;spark-submit \&#10;  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3 \&#10;  --class bronze.RawVaultETL \&#10;  --master yarn \&#10;  target/scala-2.12/data-vault-modeling-etl-1.0.0.jar&#10;```&#10;&#10;**Trade-off**: Smaller JAR (~15 MB) but requires `--packages` flag or pre-installed Iceberg in cluster.&#10;&#10;---&#10;&#10;## CI/CD Integration&#10;&#10;### Jenkins Pipeline&#10;&#10;```groovy&#10;pipeline {&#10;    agent any&#10;    stages {&#10;        stage('Build') {&#10;            steps {&#10;                sh 'sbt clean assembly'&#10;            }&#10;        }&#10;        stage('Test') {&#10;            steps {&#10;                sh 'sbt test'&#10;            }&#10;        }&#10;        stage('Deploy') {&#10;            steps {&#10;                sh 'scp target/scala-2.12/data-vault-modeling-etl-1.0.0.jar user@edge-node:/opt/etl/'&#10;            }&#10;        }&#10;        stage('Run ETL') {&#10;            steps {&#10;                sh '''&#10;                    ssh user@edge-node &quot;spark-submit \&#10;                      --class bronze.RawVaultETL \&#10;                      --master yarn \&#10;                      /opt/etl/data-vault-modeling-etl-1.0.0.jar \&#10;                      --mode incremental&quot;&#10;                '''&#10;            }&#10;        }&#10;    }&#10;}&#10;```&#10;&#10;### GitLab CI&#10;&#10;```yaml&#10;build:&#10;  stage: build&#10;  script:&#10;    - sbt clean assembly&#10;  artifacts:&#10;    paths:&#10;      - target/scala-2.12/data-vault-modeling-etl-1.0.0.jar&#10;&#10;deploy:&#10;  stage: deploy&#10;  script:&#10;    - scp target/scala-2.12/data-vault-modeling-etl-1.0.0.jar $EDGE_NODE:/opt/etl/&#10;    &#10;run-etl:&#10;  stage: run&#10;  script:&#10;    - ssh $EDGE_NODE &quot;spark-submit --class bronze.RawVaultETL --master yarn /opt/etl/data-vault-modeling-etl-1.0.0.jar --mode incremental&quot;&#10;```&#10;&#10;---&#10;&#10;## Summary&#10;&#10;### Build Command&#10;```bash&#10;sbt clean assembly&#10;```&#10;&#10;### Local Execution&#10;```bash&#10;spark-submit --class bronze.RawVaultETL --master local[*] target/scala-2.12/data-vault-modeling-etl-1.0.0.jar --mode full&#10;```&#10;&#10;### Cluster Execution&#10;```bash&#10;spark-submit --class bronze.RawVaultETL --master yarn --deploy-mode cluster /opt/etl/data-vault-modeling-etl-1.0.0.jar --mode incremental&#10;```&#10;&#10;### Key Benefits&#10;- ✅ Single JAR for all environments&#10;- ✅ No `--packages` flag needed&#10;- ✅ Iceberg included, Spark/Hadoop from runtime&#10;- ✅ ~30 MB JAR size&#10;- ✅ Configuration via properties file or environment variables&#10;&#10;---&#10;&#10;*Last Updated: 2026-01-07*&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>