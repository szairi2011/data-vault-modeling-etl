# Architecture & Design Guide

**Complete architectural reference for the Data Vault 2.0 banking data warehouse.**

---

## ğŸ“‹ Table of Contents

### Part I: Architecture Overview
- [Data Flow Architecture](#data-flow-architecture)
- [Technology Stack](#technology-stack)
- [Design Principles](#design-principles)

### Part II: Data Models
- [Source System (ERM)](#source-system-erm)
- [Data Vault Model (Bronze)](#data-vault-model-bronze)
- [Business Vault Model (Silver)](#business-vault-model-silver)
- [Dimensional Model (Gold)](#dimensional-model-gold)
- [Semantic Layer](#semantic-layer)

### Part III: Design Decisions
- [Why Data Vault 2.0](#why-data-vault-20)
- [Why Apache Avro](#why-apache-avro)
- [Why Apache NiFi](#why-apache-nifi)
- [Why Apache Iceberg](#why-apache-iceberg)
- [Why Multi-Layer Architecture](#why-multi-layer-architecture)

### Part IV: Performance & Optimization
- [Query Engine Comparison](#query-engine-comparison)

---

## PART I: ARCHITECTURE OVERVIEW

---

## Data Flow Architecture

### High-Level System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLETE DATA PIPELINE ARCHITECTURE                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OPERATIONAL SYSTEM   â”‚
â”‚ (PostgreSQL)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ banking_source DB    â”‚
â”‚ - 3NF normalized     â”‚
â”‚ - OLTP optimized     â”‚
â”‚ - Frequent changes   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ JDBC Connection
         â”‚ Incremental CDC (updated_at column)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EXTRACTION & VALIDATION LAYER                                            â”‚
â”‚ (Apache NiFi 2.7.2)                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚ QueryDatabaseTable  â”‚â”€â”€â”€â”€â†’â”‚ ConvertRecord    â”‚                      â”‚
â”‚  â”‚ Record              â”‚     â”‚ (JSON â†’ Avro)    â”‚                      â”‚
â”‚  â”‚                     â”‚     â”‚                  â”‚                      â”‚
â”‚  â”‚ SQL: SELECT *       â”‚     â”‚ Schema Validate  â”‚                      â”‚
â”‚  â”‚ WHERE updated_at >  â”‚     â”‚ against .avsc    â”‚                      â”‚
â”‚  â”‚   [last_value]      â”‚     â”‚                  â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                       â”‚                                  â”‚
â”‚                                       â”‚ Schema-validated Avro binary     â”‚
â”‚                                       â†“                                  â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                              â”‚ PutFile          â”‚                       â”‚
â”‚                              â”‚ warehouse/       â”‚                       â”‚
â”‚                              â”‚ staging/         â”‚                       â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                          â”‚
â”‚  Schemas: nifi/schemas/*.avsc (customer, account, transaction_*)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Avro Files (binary, schema-embedded)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGING AREA         â”‚
â”‚ (File System)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ warehouse/staging/   â”‚
â”‚ â”œâ”€â”€ customer/*.avro  â”‚
â”‚ â”œâ”€â”€ account/*.avro   â”‚
â”‚ â””â”€â”€ transaction_*    â”‚
â”‚     /*.avro          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Spark Read (Avro format)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAW VAULT LAYER (BRONZE)                                                 â”‚
â”‚ (Apache Spark 3.5 + Apache Iceberg)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ AvroReader   â”‚â”€â”€â”€â†’â”‚ Hash Key     â”‚â”€â”€â”€â†’â”‚ Data Vault   â”‚             â”‚
â”‚  â”‚              â”‚    â”‚ Generator    â”‚    â”‚ Load Logic   â”‚             â”‚
â”‚  â”‚ - Read Avro  â”‚    â”‚              â”‚    â”‚              â”‚             â”‚
â”‚  â”‚ - Validate   â”‚    â”‚ MD5(bus_key) â”‚    â”‚ - Hub Load   â”‚             â”‚
â”‚  â”‚ - Enrich     â”‚    â”‚              â”‚    â”‚ - Sat Load   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ - Link Load  â”‚             â”‚
â”‚                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                   â”‚                      â”‚
â”‚                                                   â†“                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ ICEBERG TABLES (Bronze Schema)                         â”‚            â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚  â”‚ HUBS:                                                  â”‚            â”‚
â”‚  â”‚ - hub_customer (customer_hash_key, customer_id, ...)  â”‚            â”‚
â”‚  â”‚ - hub_account (account_hash_key, account_id, ...)     â”‚            â”‚
â”‚  â”‚ - hub_transaction (transaction_hash_key, ...)         â”‚            â”‚
â”‚  â”‚                                                        â”‚            â”‚
â”‚  â”‚ SATELLITES:                                            â”‚            â”‚
â”‚  â”‚ - sat_customer (attributes, valid_from, valid_to)     â”‚            â”‚
â”‚  â”‚ - sat_account (attributes, valid_from, valid_to)      â”‚            â”‚
â”‚  â”‚                                                        â”‚            â”‚
â”‚  â”‚ LINKS:                                                 â”‚            â”‚
â”‚  â”‚ - link_customer_account (customer â†” account)          â”‚            â”‚
â”‚  â”‚ - link_transaction_item (transaction â†” item)          â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                          â”‚
â”‚  Features: Schema evolution, full history, hash-based joins              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Spark SQL (Join Hubs + Satellites)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BUSINESS VAULT LAYER (SILVER)                                            â”‚
â”‚ (Apache Spark 3.5 + Apache Iceberg)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ PIT Builder  â”‚                    â”‚ Bridge       â”‚                  â”‚
â”‚  â”‚              â”‚                    â”‚ Builder      â”‚                  â”‚
â”‚  â”‚ Join Hub +   â”‚                    â”‚              â”‚                  â”‚
â”‚  â”‚ Satellite    â”‚                    â”‚ Multi-hop    â”‚                  â”‚
â”‚  â”‚ @ snapshot   â”‚                    â”‚ relationshipsâ”‚                  â”‚
â”‚  â”‚ date         â”‚                    â”‚ + aggregates â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚         â”‚                                     â”‚                         â”‚
â”‚         â†“                                     â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ ICEBERG TABLES (Silver Schema)                         â”‚            â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚  â”‚ PIT TABLES:                                            â”‚            â”‚
â”‚  â”‚ - pit_customer (flattened current attributes)          â”‚            â”‚
â”‚  â”‚ - pit_account (flattened current attributes)           â”‚            â”‚
â”‚  â”‚                                                        â”‚            â”‚
â”‚  â”‚ BRIDGE TABLES:                                         â”‚            â”‚
â”‚  â”‚ - bridge_customer_account (pre-joined + metrics)       â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                          â”‚
â”‚  Features: Query optimization, denormalization, aggregation              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Spark SQL (SCD Type 2 + Star Schema)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DIMENSIONAL MODEL LAYER (GOLD)                                           â”‚
â”‚ (Apache Spark 3.5 + Apache Iceberg)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ SCD Type 2   â”‚                    â”‚ Fact Builder â”‚                  â”‚
â”‚  â”‚ Handler      â”‚                    â”‚              â”‚                  â”‚
â”‚  â”‚              â”‚                    â”‚ Lookup dim   â”‚                  â”‚
â”‚  â”‚ Track        â”‚                    â”‚ keys, calc   â”‚                  â”‚
â”‚  â”‚ dimension    â”‚                    â”‚ metrics      â”‚                  â”‚
â”‚  â”‚ history      â”‚                    â”‚              â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚         â”‚                                     â”‚                         â”‚
â”‚         â†“                                     â†“                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ ICEBERG TABLES (Gold Schema - Star Schema)             â”‚            â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚  â”‚ DIMENSIONS (SCD Type 2):                               â”‚            â”‚
â”‚  â”‚ - dim_customer (surrogate key, SCD history)            â”‚            â”‚
â”‚  â”‚ - dim_account (surrogate key, SCD history)             â”‚            â”‚
â”‚  â”‚ - dim_date (date dimension, 10 years)                  â”‚            â”‚
â”‚  â”‚ - dim_product, dim_branch                              â”‚            â”‚
â”‚  â”‚                                                        â”‚            â”‚
â”‚  â”‚ FACTS (Additive):                                      â”‚            â”‚
â”‚  â”‚ - fact_transaction (metrics + dim keys)                â”‚            â”‚
â”‚  â”‚ - fact_account_balance (daily snapshots)               â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                          â”‚
â”‚  Features: BI-friendly, fast aggregations, SCD Type 2 history            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ JDBC / Spark SQL
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BI & ANALYTICS       â”‚
â”‚ (Tableau, Power BI)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - Customer 360       â”‚
â”‚ - Transaction Trends â”‚
â”‚ - Account Balances   â”‚
â”‚ - Revenue Reports    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Interaction Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  JDBC   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Avro   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Spark  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚PostgreSQLâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  NiFi    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ Staging  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  Bronze  â”‚
â”‚  (OLTP)  â”‚  Query  â”‚(Validate)â”‚  Write  â”‚  (Files) â”‚  Read   â”‚(Raw Vault)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                      â”‚
                                                                      â†“
                                                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                 â”‚  Silver  â”‚
                                                                 â”‚(Optimized)â”‚
                                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                      â”‚
                                                                      â†“
                                                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                 â”‚   Gold   â”‚
                                                                 â”‚(Star Schema)â”‚
                                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Technology Stack

### Data Ingestion
- **Apache NiFi 2.7.2**
  - Visual data flow design
  - Built-in CDC support (QueryDatabaseTableRecord)
  - Schema validation (ConvertRecord)
  - No code required for basic flows

### Data Format
- **Apache Avro**
  - Binary format (compact storage)
  - Embedded schemas (self-describing)
  - Schema evolution support
  - Native Spark integration

### Data Processing
- **Apache Spark 3.5**
  - Distributed processing
  - DataFrame API for transformations
  - Catalyst optimizer
  - Native Avro reader

### Data Storage
- **Apache Iceberg**
  - ACID transactions
  - Schema evolution
  - Time travel queries
  - Hidden partitioning

### Source Database
- **PostgreSQL 12+**
  - Operational data store
  - 3NF normalized
  - Supports JDBC connectivity

### Platform
- **Windows Native**
  - No Docker required
  - PowerShell scripting
  - Native NiFi installation

---

## Design Principles

### 1. Separation of Concerns
Each layer has a single responsibility:
- **NiFi:** Extract and validate
- **Bronze:** Store raw history
- **Silver:** Optimize queries
- **Gold:** Serve analytics

### 2. Schema Evolution Resilience
The system handles source schema changes gracefully:
- Avro provides type-safe schema contracts
- Data Vault absorbs new columns automatically
- Historical queries remain unaffected

### 3. Single Source of Truth for Schema Validation
Schema validation logic derives from Avro schema definitions (`nifi/schemas/*.avsc`):
- **No hardcoded field lists** - Required fields extracted dynamically from `.avsc` files
- **AvroReader utility** - Loads schemas at runtime and caches them for performance
- **Validation rules** - Fields without `null` union or `default` value are considered required
- **Consistency guaranteed** - Schema changes in `.avsc` automatically reflected in validation
- **Clear error messages** - Validation failures point to specific `.avsc` file for correction

**Example:** When `customer.avsc` is updated to add a new required field, the Spark ETL automatically enforces it without code changes.

### 4. Auditability
Every data point is traceable:
- Load timestamps on all records
- valid_from/valid_to tracking in Satellites
- Source system tracking in Hubs
- Full lineage from source to analytics

### 5. Performance Through Layering
Each layer optimizes for different access patterns:
- Bronze: Write-optimized (append-only)
- Silver: Read-optimized (pre-joined)
- Gold: Aggregate-optimized (star schema)

### 6. Decoupling
Components are loosely coupled:
- NiFi writes files, Spark reads files (no direct coupling)
- Each layer can be rebuilt independently
- Technology swaps are easier (e.g., replace NiFi with Kafka)

---

## PART II: DATA MODELS

---

## Source System (ERM)

### Entity-Relationship Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BANKING SOURCE SYSTEM (3NF)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   CUSTOMER   â”‚
                     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                     â”‚ customer_id  â”‚ PK
                     â”‚ customer_typeâ”‚ (INDIVIDUAL, BUSINESS)
                     â”‚ first_name   â”‚
                     â”‚ last_name    â”‚
                     â”‚ email        â”‚ UNIQUE
                     â”‚ phone        â”‚
                     â”‚ address      â”‚
                     â”‚ city, state  â”‚
                     â”‚ zip_code     â”‚
                     â”‚ customer_status â”‚ (ACTIVE, INACTIVE, CLOSED)
                     â”‚ created_at   â”‚
                     â”‚ updated_at   â”‚ (CDC tracking)
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ 1
                            â”‚
                            â”‚ owns
                            â”‚
                            â”‚ N
                            â†“
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   ACCOUNT    â”‚
                     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                     â”‚ account_id   â”‚ PK
                     â”‚ customer_id  â”‚ FK â†’ customer
                     â”‚ product_id   â”‚ FK â†’ product
                     â”‚ branch_id    â”‚ FK â†’ branch
                     â”‚ account_numberâ”‚ UNIQUE
                     â”‚ account_type â”‚ (CHECKING, SAVINGS, CREDIT_CARD, LOAN)
                     â”‚ account_statusâ”‚ (OPEN, CLOSED, SUSPENDED)
                     â”‚ balance      â”‚ DECIMAL(15,2)
                     â”‚ currency     â”‚
                     â”‚ opened_date  â”‚
                     â”‚ closed_date  â”‚
                     â”‚ created_at   â”‚
                     â”‚ updated_at   â”‚ (CDC tracking)
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ 1
                            â”‚
                            â”‚ has
                            â”‚
                            â”‚ N
                            â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  TRANSACTION_HEADER      â”‚
              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
              â”‚ transaction_id           â”‚ PK
              â”‚ account_id               â”‚ FK â†’ account
              â”‚ transaction_number       â”‚ UNIQUE
              â”‚ transaction_date         â”‚
              â”‚ transaction_type         â”‚ (DEPOSIT, WITHDRAWAL, PAYMENT, TRANSFER)
              â”‚ transaction_status       â”‚ (COMPLETED, PENDING, FAILED)
              â”‚ total_amount             â”‚ DECIMAL(15,2)
              â”‚ currency                 â”‚
              â”‚ description              â”‚
              â”‚ created_at               â”‚
              â”‚ updated_at               â”‚ (CDC tracking)
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ 1
                            â”‚
                            â”‚ contains
                            â”‚
                            â”‚ N
                            â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  TRANSACTION_ITEM        â”‚
              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
              â”‚ item_id                  â”‚ PK
              â”‚ transaction_id           â”‚ FK â†’ transaction_header
              â”‚ category_id              â”‚ FK â†’ category
              â”‚ item_sequence            â”‚ (order within transaction)
              â”‚ item_type                â”‚
              â”‚ merchant_name            â”‚
              â”‚ merchant_category        â”‚
              â”‚ item_amount              â”‚ DECIMAL(15,2)
              â”‚ item_description         â”‚
              â”‚ created_at               â”‚
              â”‚ updated_at               â”‚ (CDC tracking)
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        REFERENCE TABLES                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PRODUCT    â”‚       â”‚   BRANCH     â”‚       â”‚  CATEGORY    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ product_id   â”‚       â”‚ branch_id    â”‚       â”‚ category_id  â”‚
â”‚ product_code â”‚       â”‚ branch_code  â”‚       â”‚ category_nameâ”‚
â”‚ product_name â”‚       â”‚ branch_name  â”‚       â”‚ parent_id    â”‚ (hierarchical)
â”‚ product_type â”‚       â”‚ address      â”‚       â”‚ path         â”‚ (materialized path)
â”‚ description  â”‚       â”‚ city, state  â”‚       â”‚ level        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ manager      â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multi-Item Transaction Example

**Business Scenario:** Customer pays multiple bills in one transaction (like an e-commerce order).

```
transaction_header:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ transaction_id â”‚ account_id  â”‚ total_amount â”‚ type       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1001           â”‚ 101         â”‚ 250.00       â”‚ PAYMENT    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â”‚ contains
                        â†“
transaction_item:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ item_idâ”‚ transaction_id â”‚ merchant_nameâ”‚ item_amount â”‚ descriptionâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2001   â”‚ 1001           â”‚ Con Edison   â”‚ 100.00      â”‚ Electricityâ”‚
â”‚ 2002   â”‚ 1001           â”‚ Water Dept   â”‚ 50.00       â”‚ Water Bill â”‚
â”‚ 2003   â”‚ 1001           â”‚ Comcast      â”‚ 100.00      â”‚ Internet   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why This Pattern?**
- Matches real-world bill payments, shopping carts, split transactions
- Enables item-level analytics (which merchants are most used?)
- Supports partial refunds, item-level taxation

### Normalization Level

**3rd Normal Form (3NF):**
- âœ… No repeating groups (transaction items in separate table)
- âœ… All non-key attributes depend on the key (customer_id â†’ email, not on other attributes)
- âœ… No transitive dependencies (branch info not in account table)

**Why 3NF for OLTP:**
- Minimizes data redundancy
- Optimizes for INSERT/UPDATE/DELETE
- Maintains data integrity through foreign keys

---

## Data Vault Model (Bronze)

### Data Vault 2.0 Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BRONZE LAYER - RAW VAULT                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              HUBS                                      â”‚
â”‚  (Store unique business entities - deduplicated)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

hub_customer                    hub_account                   hub_transaction
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ customer_hash_key â”‚ PK       â”‚ account_hash_key  â”‚ PK      â”‚ transaction_hash  â”‚ PK
â”‚ customer_id       â”‚ BK       â”‚ account_id        â”‚ BK      â”‚ transaction_id    â”‚ BK
â”‚ load_timestamp    â”‚          â”‚ load_timestamp    â”‚         â”‚ load_timestamp    â”‚
â”‚ record_source     â”‚          â”‚ record_source     â”‚         â”‚ record_source     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BK = Business Key (from source system)
PK = Primary Key (MD5 hash of business key)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           SATELLITES                                   â”‚
â”‚  (Store attributes with full history - SCD Type 2)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

sat_customer
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ customer_hash_key â”‚ PK, FK â†’ hub_customer
â”‚ load_timestamp    â”‚ PK (part of composite key)
â”‚ customer_type     â”‚ INDIVIDUAL, BUSINESS
â”‚ first_name        â”‚
â”‚ last_name         â”‚
â”‚ email             â”‚
â”‚ phone             â”‚
â”‚ address           â”‚
â”‚ city              â”‚
â”‚ state             â”‚
â”‚ zip_code          â”‚
â”‚ customer_status   â”‚ ACTIVE, INACTIVE, CLOSED
â”‚ valid_from        â”‚ Timestamp when version became active
â”‚ valid_to          â”‚ Timestamp when superseded (NULL = current)
â”‚ hash_diff         â”‚ MD5(all_attributes) for change detection
â”‚ record_source     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

sat_account
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ account_hash_key  â”‚ PK, FK â†’ hub_account
â”‚ load_timestamp    â”‚ PK
â”‚ account_number    â”‚
â”‚ account_type      â”‚
â”‚ account_status    â”‚
â”‚ balance           â”‚
â”‚ currency          â”‚
â”‚ opened_date       â”‚
â”‚ closed_date       â”‚
â”‚ valid_from        â”‚
â”‚ valid_to          â”‚
â”‚ hash_diff         â”‚
â”‚ record_source     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

sat_transaction
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ transaction_hash  â”‚ PK, FK â†’ hub_transaction
â”‚ load_timestamp    â”‚ PK
â”‚ transaction_numberâ”‚
â”‚ transaction_date  â”‚
â”‚ transaction_type  â”‚
â”‚ transaction_statusâ”‚
â”‚ total_amount      â”‚
â”‚ currency          â”‚
â”‚ description       â”‚
â”‚ valid_from        â”‚
â”‚ valid_to          â”‚
â”‚ hash_diff         â”‚
â”‚ record_source     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             LINKS                                      â”‚
â”‚  (Store relationships between entities)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

link_customer_account
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ link_hash_key     â”‚ PK = MD5(customer_hash_key + account_hash_key)
â”‚ customer_hash_key â”‚ FK â†’ hub_customer
â”‚ account_hash_key  â”‚ FK â†’ hub_account
â”‚ load_timestamp    â”‚
â”‚ record_source     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

link_transaction_item
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ link_hash_key         â”‚ PK = MD5(transaction_hash + item_hash)
â”‚ transaction_hash_key  â”‚ FK â†’ hub_transaction
â”‚ item_hash_key         â”‚ FK â†’ hub_transaction_item
â”‚ load_timestamp        â”‚
â”‚ record_source         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hash Key Generation

**Why MD5 hashing?**
- Deterministic (same input â†’ same hash)
- Fixed length (32 hex characters)
- Fast to compute
- Enables hash-based joins (no integer lookups)

**Example:**
```scala
// Customer with customer_id = 1
val businessKey = "1"
val hashKey = md5(businessKey)
// Result: "c4ca4238a0b923820dcc509a6f75849b"

// Customer-Account link
val compositeKey = "c4ca4238a0b923820dcc509a6f75849b" + "550e8400-e29b-41d4-a716-446655440000"
val linkHash = md5(compositeKey)
```

### History Tracking with valid_from/valid_to

**Initial Load:**
```sql
INSERT INTO sat_customer (
  customer_hash_key, email, customer_status,
  valid_from, valid_to
) VALUES (
  'c4ca4238...', 'john@example.com', 'ACTIVE',
  '2025-12-20 10:00:00', NULL  -- NULL = current version
);
```

**Schema Evolution (loyalty_tier added):**
```sql
-- End-date old version
UPDATE sat_customer
SET valid_to = '2025-12-20 14:00:00'
WHERE customer_hash_key = 'c4ca4238...'
  AND valid_to IS NULL;

-- Insert new version with new column
INSERT INTO sat_customer (
  customer_hash_key, email, customer_status, loyalty_tier,
  valid_from, valid_to
) VALUES (
  'c4ca4238...', 'john@example.com', 'ACTIVE', 'GOLD',
  '2025-12-20 14:00:00', NULL
);
```

**Querying History:**
```sql
-- Get current version
SELECT * FROM sat_customer WHERE valid_to IS NULL;

-- Get version as of specific date
SELECT * FROM sat_customer
WHERE valid_from <= '2025-12-20 12:00:00'
  AND (valid_to > '2025-12-20 12:00:00' OR valid_to IS NULL);

-- Get full history for one customer
SELECT * FROM sat_customer
WHERE customer_hash_key = 'c4ca4238...'
ORDER BY valid_from;
```

### Data Vault Benefits Demonstrated

**1. Schema Evolution:**
```
Source adds loyalty_tier â†’ Satellite automatically gets new column
Old records: loyalty_tier = NULL
New records: loyalty_tier populated
Existing queries: still work (don't reference loyalty_tier)
```

**2. Full Audit Trail:**
```
Who: record_source (which ETL job)
What: hash_diff (what changed)
When: valid_from/valid_to (temporal tracking)
Where: load_timestamp (which load batch)
```

**3. Deduplication:**
```
Hub only stores unique business keys
If customer_id = 1 loaded twice â†’ only one hub_customer row
Satellite stores both versions with different valid_from
```

---

## Business Vault Model (Silver)

### Point-in-Time (PIT) Tables

**Purpose:** Flatten Data Vault for query performance

```
pit_customer (snapshot_date = 2025-12-20)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ customer_id â”‚ emailâ”‚ f_name  â”‚ l_name â”‚ status     â”‚ loyalty_tier â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1           â”‚john@ â”‚John     â”‚Doe     â”‚ACTIVE      â”‚GOLD          â”‚
â”‚ 2           â”‚jane@ â”‚Jane     â”‚Smith   â”‚ACTIVE      â”‚PLATINUM      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†‘ All attributes flattened from hub_customer + sat_customer
```

**How PIT is Built:**
```scala
// Pseudo-code
val pit = hub_customer
  .join(sat_customer, "customer_hash_key")
  .filter("valid_to IS NULL")  // Current version only
  .select(
    col("customer_id"),
    col("email"),
    col("first_name"),
    col("last_name"),
    col("customer_status"),
    col("loyalty_tier"),
    lit(current_date).as("snapshot_date")
  )
  .write.format("iceberg")
  .mode("overwrite")
  .save("silver.pit_customer")
```

**Query Comparison:**
```sql
-- Without PIT (Bronze - complex)
SELECT h.customer_id, s.email, s.customer_status
FROM bronze.hub_customer h
JOIN bronze.sat_customer s ON h.customer_hash_key = s.customer_hash_key
WHERE s.valid_to IS NULL;

-- With PIT (Silver - simple)
SELECT customer_id, email, customer_status
FROM silver.pit_customer
WHERE snapshot_date = CURRENT_DATE;
```

### Bridge Tables

**Purpose:** Pre-compute relationships and aggregates

```
bridge_customer_account
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ customer_id â”‚ account_id â”‚ balance  â”‚ account_countâ”‚ total_bal  â”‚ is_primaryâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1           â”‚ 101        â”‚ 5000     â”‚ 2            â”‚ 15000      â”‚ false     â”‚
â”‚ 1           â”‚ 102        â”‚ 10000    â”‚ 2            â”‚ 15000      â”‚ true      â”‚
â”‚ 2           â”‚ 201        â”‚ 50000    â”‚ 1            â”‚ 50000      â”‚ true      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†‘ Pre-joined: customer â†” account relationship
              â†‘ Pre-aggregated: account count, total balance
              â†‘ Business rule: primary = highest balance
```

**How Bridge is Built:**
```scala
// Pseudo-code
val bridge = hub_customer
  .join(link_customer_account, "customer_hash_key")
  .join(hub_account, "account_hash_key")
  .join(sat_account.filter("valid_to IS NULL"), "account_hash_key")
  .groupBy("customer_id")
  .agg(
    count("account_id").as("account_count"),
    sum("balance").as("total_balance")
  )
  .withColumn("is_primary", 
    row_number().over(Window.partitionBy("customer_id").orderBy(desc("balance"))) === 1
  )
```

---

## Dimensional Model (Gold)

### Star Schema Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GOLD LAYER - STAR SCHEMA                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                              DIMENSIONS
                                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚                         â”‚
        â†“                         â†“                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ dim_customer  â”‚        â”‚  dim_account  â”‚        â”‚   dim_date    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ customer_key  â”‚ PK     â”‚ account_key   â”‚ PK     â”‚ date_key      â”‚ PK
â”‚ customer_id   â”‚ NK     â”‚ account_id    â”‚ NK     â”‚ date          â”‚
â”‚ customer_type â”‚        â”‚ account_type  â”‚        â”‚ year          â”‚
â”‚ full_name     â”‚        â”‚ account_statusâ”‚        â”‚ quarter       â”‚
â”‚ email         â”‚        â”‚ balance       â”‚        â”‚ month         â”‚
â”‚ phone         â”‚        â”‚ currency      â”‚        â”‚ month_name    â”‚
â”‚ address       â”‚        â”‚ customer_key  â”‚ FK     â”‚ day           â”‚
â”‚ city, state   â”‚        â”‚ product_key   â”‚ FK     â”‚ day_of_week   â”‚
â”‚ zip_code      â”‚        â”‚ branch_key    â”‚ FK     â”‚ is_weekend    â”‚
â”‚ status        â”‚        â”‚ eff_start_dateâ”‚        â”‚ is_holiday    â”‚
â”‚ loyalty_tier  â”‚        â”‚ eff_end_date  â”‚        â”‚ fiscal_year   â”‚
â”‚ eff_start_dateâ”‚        â”‚ is_current    â”‚        â”‚ fiscal_quarterâ”‚
â”‚ eff_end_date  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ is_current    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        
PK = Primary Key (surrogate)
NK = Natural Key (business key)
FK = Foreign Key


                              FACTS
                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                               â”‚
        â†“                                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ fact_transaction    â”‚                    â”‚ fact_account_balanceâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ transaction_key     â”‚ PK                 â”‚ balance_key         â”‚ PK
â”‚ customer_key        â”‚ FK â†’ dim_customer  â”‚ account_key         â”‚ FK â†’ dim_account
â”‚ account_key         â”‚ FK â†’ dim_account   â”‚ date_key            â”‚ FK â†’ dim_date
â”‚ date_key            â”‚ FK â†’ dim_date      â”‚ balance_amount      â”‚ Measure
â”‚ product_key         â”‚ FK â†’ dim_product   â”‚ available_balance   â”‚ Measure
â”‚ branch_key          â”‚ FK â†’ dim_branch    â”‚ pending_amount      â”‚ Measure
â”‚ transaction_type    â”‚ Degenerate dim     â”‚ currency            â”‚
â”‚ transaction_status  â”‚ Degenerate dim     â”‚ snapshot_timestamp  â”‚
â”‚ net_amount          â”‚ Measure            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ item_count          â”‚ Measure
â”‚ currency            â”‚
â”‚ transaction_timestampâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SCD Type 2 Implementation

**Slowly Changing Dimension Type 2** tracks historical changes by creating new rows.

**Example: Customer changes address**

**Before:**
```
dim_customer
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚customer_keyâ”‚customer_id  â”‚address  â”‚eff_start    â”‚eff_end      â”‚is_currentâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚1           â”‚101          â”‚123 Main â”‚2025-01-01   â”‚9999-12-31   â”‚true      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**After (address changed to 456 Oak):**
```
dim_customer
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚customer_keyâ”‚customer_id  â”‚address  â”‚eff_start    â”‚eff_end      â”‚is_currentâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚1           â”‚101          â”‚123 Main â”‚2025-01-01   â”‚2025-12-20   â”‚false     â”‚
â”‚2           â”‚101          â”‚456 Oak  â”‚2025-12-20   â”‚9999-12-31   â”‚true      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†‘ Same customer_id, different customer_key
```

**SCD Type 2 Logic:**
```scala
// Pseudo-code for SCD Type 2 processing
val incoming = pit_customer  // Source data
val existing = dim_customer  // Current dimension

// Detect changes
val changed = incoming.join(existing, "customer_id")
  .filter(md5(incoming_attributes) =!= md5(existing_attributes))
  .filter("is_current = true")

// End-date old versions
existing.filter("customer_id IN changed_customer_ids")
  .update(
    eff_end_date = current_date,
    is_current = false
  )

// Insert new versions
changed.select(
  next_surrogate_key.as("customer_key"),
  customer_id,
  new_attributes,
  current_date.as("eff_start_date"),
  to_date("9999-12-31").as("eff_end_date"),
  lit(true).as("is_current")
).write.mode("append").save("dim_customer")
```

**Benefits:**
- Historical accuracy: "What was customer 101's address on 2025-06-01?" â†’ 123 Main
- Fact table integrity: Old transactions point to customer_key = 1 (old address)
- New transactions point to customer_key = 2 (new address)

### Fact Table Grain

**fact_transaction grain:** One row per transaction

```
Dimensions:
- When: date_key (transaction date)
- Who: customer_key (who made transaction)
- What: account_key (which account)
- Where: branch_key (which branch)

Measures:
- net_amount (transaction value)
- item_count (number of line items)

Degenerate Dimensions:
- transaction_type (DEPOSIT, WITHDRAWAL, etc.)
- transaction_status (COMPLETED, PENDING)
```

**fact_account_balance grain:** One row per account per day

```
Dimensions:
- When: date_key (snapshot date)
- What: account_key (which account)

Measures:
- balance_amount (end-of-day balance)
- available_balance (after holds)
- pending_amount (transactions not cleared)
```

---

## Semantic Layer

### Business Views

**Purpose:** Provide business-friendly abstractions over the star schema

#### View: vw_customer_360

```sql
CREATE VIEW gold.vw_customer_360 AS
SELECT 
  c.customer_id,
  c.full_name,
  c.email,
  c.customer_type,
  c.loyalty_tier,
  COUNT(DISTINCT a.account_key) as account_count,
  SUM(a.balance) as total_balance,
  COUNT(DISTINCT f.transaction_key) as transaction_count,
  SUM(f.net_amount) as lifetime_value,
  MAX(f.transaction_timestamp) as last_transaction_date
FROM gold.dim_customer c
LEFT JOIN gold.dim_account a ON c.customer_key = a.customer_key
  AND a.is_current = true
LEFT JOIN gold.fact_transaction f ON c.customer_key = f.customer_key
WHERE c.is_current = true
GROUP BY 
  c.customer_id,
  c.full_name,
  c.email,
  c.customer_type,
  c.loyalty_tier;
```

**Business Use:** Customer service reps query one view to see complete customer profile

#### View: vw_daily_transactions

```sql
CREATE VIEW gold.vw_daily_transactions AS
SELECT 
  d.date,
  d.day_of_week,
  d.month_name,
  COUNT(f.transaction_key) as transaction_count,
  SUM(f.net_amount) as total_volume,
  AVG(f.net_amount) as avg_transaction_size,
  COUNT(DISTINCT f.customer_key) as unique_customers
FROM gold.fact_transaction f
JOIN gold.dim_date d ON f.date_key = d.date_key
GROUP BY 
  d.date,
  d.day_of_week,
  d.month_name;
```

**Business Use:** Daily transaction monitoring dashboard

#### View: vw_account_profitability

```sql
CREATE VIEW gold.vw_account_profitability AS
SELECT 
  a.account_id,
  a.account_type,
  c.customer_id,
  c.full_name,
  p.product_name,
  b.branch_name,
  ab.balance_amount as current_balance,
  COUNT(f.transaction_key) as monthly_transactions,
  -- Simplified profitability (in real-world: fees - costs)
  COUNT(f.transaction_key) * 0.50 as estimated_monthly_revenue
FROM gold.dim_account a
JOIN gold.dim_customer c ON a.customer_key = c.customer_key
JOIN gold.dim_product p ON a.product_key = p.product_key
JOIN gold.dim_branch b ON a.branch_key = b.branch_key
JOIN gold.fact_account_balance ab ON a.account_key = ab.account_key
  AND ab.date_key = (SELECT MAX(date_key) FROM gold.dim_date)
LEFT JOIN gold.fact_transaction f ON a.account_key = f.account_key
  AND f.date_key >= (SELECT date_key FROM gold.dim_date WHERE date = CURRENT_DATE - INTERVAL '30 days')
WHERE a.is_current = true
GROUP BY 
  a.account_id,
  a.account_type,
  c.customer_id,
  c.full_name,
  p.product_name,
  b.branch_name,
  ab.balance_amount;
```

**Business Use:** Identify low-value accounts for optimization

### Metric Definitions

**Customer Lifetime Value (CLV):**
```sql
SUM(fact_transaction.net_amount) WHERE customer_key = X
```

**Average Transaction Size:**
```sql
AVG(fact_transaction.net_amount)
```

**Customer Retention Rate (monthly):**
```sql
COUNT(DISTINCT customer_key this month) / 
COUNT(DISTINCT customer_key last month)
```

**Account Balance Trend:**
```sql
SELECT 
  date,
  AVG(balance_amount) OVER (ORDER BY date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) as moving_avg_30d
FROM fact_account_balance
```

---

## PART III: DESIGN DECISIONS

---

## Why Data Vault 2.0

### Problem with Traditional Star Schema

**Scenario:** Marketing launches loyalty program, adds `loyalty_tier` to customer table

**Impact on traditional data warehouse:**
1. **ETL breaks** - Hardcoded column positions fail
2. **Dashboards fail** - Missing column in SELECT *
3. **Historical data lost** - Can't query "What tier was customer X before change?"
4. **Emergency weekend work** - Fix ETL, redeploy, validate

### Data Vault Solution

**Same scenario with Data Vault:**
1. **ETL auto-adapts** - New column added to Satellite
2. **Dashboards keep working** - Old queries don't reference new column
3. **History preserved** - Old records have loyalty_tier = NULL
4. **Zero downtime** - Incremental load handles gracefully

### Data Vault Benefits

| Benefit | How Achieved |
|---------|-------------|
| **Schema evolution resilience** | Satellites absorb new columns automatically |
| **Full audit trail** | valid_from/valid_to + load_timestamp on all records |
| **Parallel loading** | Hubs, Links, Satellites loaded independently |
| **Incremental friendly** | Hash keys enable efficient deduplication |
| **Source agnostic** | Same vault structure for any source system |

### When NOT to Use Data Vault

âŒ **Simple reporting (< 5 tables)** - Star schema is simpler  
âŒ **No schema changes expected** - Overhead not justified  
âŒ **Real-time streaming** - Data Vault adds latency  
âŒ **Small team, no data modeler** - Requires understanding of patterns  

---

## Why Apache Avro

### Problem with JSON for Data Staging

**JSON limitations:**
- No embedded schema (requires external schema definition)
- Verbose (text-based, large files)
- No type safety (everything is string)
- Manual validation required

### Avro Solution

**Avro advantages:**
1. **Embedded schema** - Schema travels with data
2. **Binary format** - 40-60% smaller than JSON
3. **Type safe** - Fields have defined types (int, string, timestamp)
4. **Schema evolution** - Add/remove fields with compatibility rules

### Avro Schema Example

```json
{
  "type": "record",
  "name": "Customer",
  "namespace": "com.banking.source",
  "fields": [
    {"name": "customer_id", "type": "int"},
    {"name": "email", "type": "string"},
    {
      "name": "loyalty_tier",
      "type": ["null", "string"],
      "default": null,
      "doc": "Added in v2 - optional for backward compatibility"
    }
  ]
}
```

**Backward compatibility:**
- Old readers (v1) ignore `loyalty_tier`
- New readers (v2) see `loyalty_tier` (NULL if from old writer)

### Avro vs Parquet

| Aspect | Avro | Parquet |
|--------|------|---------|
| **Format** | Row-based | Columnar |
| **Use case** | Streaming, staging | Analytics queries |
| **Schema** | Embedded | Embedded |
| **Compression** | Good | Excellent |
| **Write speed** | Fast | Slower |
| **Read (full row)** | Fast | Slower |
| **Read (few columns)** | Slower | Fast |

**Our choice:** Avro for staging (row-based writes from NiFi), Iceberg/Parquet for warehouse (columnar analytics)

---

## Why Apache NiFi

### Problem with Custom ETL Code

**Traditional approach:** Write Python/Scala scripts for extraction

**Problems:**
- Code maintenance (every source change = code change)
- No visual monitoring
- Hard to debug data flow issues
- Developers become bottleneck

### NiFi Solution

**Visual data flow design:**
```
QueryDatabaseTableRecord â”€â”€â†’ ConvertRecord â”€â”€â†’ PutFile
    (configure in UI)        (schema validate)   (write Avro)
```

**Benefits:**
1. **No code for basic flows** - Configure in UI
2. **Built-in CDC** - QueryDatabaseTableRecord tracks state
3. **Schema validation** - ConvertRecord enforces Avro schema
4. **Real-time monitoring** - See data flowing through processors
5. **Backpressure handling** - Automatic queue management

### NiFi vs Alternatives

| Tool | Strength | Weakness |
|------|----------|----------|
| **NiFi** | Visual, real-time, CDC | Learning curve |
| **Airflow** | Orchestration | Not for data movement |
| **Kafka** | Streaming | Requires infrastructure |
| **Custom scripts** | Flexible | Maintenance burden |

**Our choice:** NiFi for extraction layer (visual, CDC built-in), Spark for transformations (complex logic)

---

## Why Apache Iceberg

### Problem with Hive Tables

**Traditional Hive limitations:**
- No ACID transactions (can't update/delete reliably)
- Schema evolution difficult (ALTER TABLE required)
- No time travel (can't query historical versions)
- Hidden partitioning weak (manual partition management)

### Iceberg Solution

**ACID transactions:**
```scala
// Atomic update - all or nothing
spark.sql("""
  UPDATE bronze.sat_customer
  SET valid_to = CURRENT_TIMESTAMP
  WHERE customer_hash_key = 'abc123' AND valid_to IS NULL
""")
// If fails, rollback automatic
```

**Schema evolution:**
```scala
// Add column without ALTER TABLE
df.withColumn("loyalty_tier", lit(null))
  .write.format("iceberg")
  .mode("append")
  .save("bronze.sat_customer")
// Schema automatically evolves
```

**Time travel:**
```scala
// Query table as it was 7 days ago
spark.read.format("iceberg")
  .option("as-of-timestamp", "2025-12-13 10:00:00")
  .table("bronze.sat_customer")
```

**Hidden partitioning:**
```scala
// Partition by date, but users don't specify it in queries
// Iceberg handles automatically
spark.sql("SELECT * FROM bronze.sat_customer WHERE valid_from > '2025-12-01'")
// Iceberg prunes partitions automatically
```

### Iceberg vs Alternatives

| Format | ACID | Schema Evolution | Time Travel | Partitioning |
|--------|------|-----------------|-------------|--------------|
| **Iceberg** | âœ… | âœ… | âœ… | Hidden |
| **Delta Lake** | âœ… | âœ… | âœ… | Manual |
| **Hive** | âŒ | Limited | âŒ | Manual |
| **Parquet** | âŒ | âŒ | âŒ | N/A |

**Our choice:** Iceberg for Data Vault (needs ACID, schema evolution) over Delta Lake (better multi-engine support)

---

## Why Multi-Layer Architecture

### Bronze â†’ Silver â†’ Gold Pattern

**Why not just load directly to Gold?**

**Problem with single-layer approach:**
- Lose raw history (can't reprocess if business logic changes)
- Performance issues (complex joins in every query)
- Tight coupling (source changes break analytics)

### Multi-Layer Benefits

**Bronze (Raw Vault):**
- **Purpose:** Immutable history
- **Benefit:** Can rebuild Silver/Gold if needed
- **Trade-off:** Complex to query directly

**Silver (Business Vault):**
- **Purpose:** Query optimization
- **Benefit:** Pre-joined tables, fast queries
- **Trade-off:** Additional processing step

**Gold (Dimensional Model):**
- **Purpose:** BI-friendly structure
- **Benefit:** BI tools understand star schema
- **Trade-off:** Less flexible than Data Vault

### Data Flow Example

**Scenario:** Calculate customer lifetime value

**Without layers (direct query):**
```sql
-- Complex, slow, couples source and analytics
SELECT 
  c.customer_id,
  SUM(t.total_amount) as ltv
FROM source.customer c
JOIN source.account a ON c.customer_id = a.customer_id
JOIN source.transaction_header t ON a.account_id = t.account_id
GROUP BY c.customer_id;
-- Problem: If source schema changes, query breaks
```

**With layers:**
```sql
-- Bronze: Raw history stored
-- Silver: Pre-joined customer â†” account in bridge table
-- Gold: Simple query on star schema
SELECT 
  customer_id,
  SUM(net_amount) as ltv
FROM gold.fact_transaction
GROUP BY customer_id;
-- Benefit: Source changes absorbed in Bronze, Gold stays stable
```

### Layer Rebuild Strategy

**If business logic changes:**
```
Bronze (unchanged) â†’ Re-run Silver ETL â†’ Re-run Gold ETL
```

**If source schema changes:**
```
Re-run Bronze ETL (auto-absorbs) â†’ Re-run Silver â†’ Re-run Gold
```

**If BI requirements change:**
```
Bronze (unchanged) â†’ Silver (unchanged) â†’ Re-model Gold
```

---

## Summary

**This architecture provides:**
âœ… **Resilience to change** - Data Vault absorbs schema evolution  
âœ… **Full auditability** - Every change tracked with timestamps  
âœ… **Performance** - Multi-layer optimization for different access patterns  
âœ… **Scalability** - Spark + Iceberg handle petabyte-scale data  
âœ… **Type safety** - Avro enforces schemas at ingestion  
âœ… **Maintainability** - NiFi visual flows + clear layer separation  

**Trade-offs accepted:**
âš ï¸ **Complexity** - More layers than simple star schema  
âš ï¸ **Latency** - Multi-layer processing adds time  
âš ï¸ **Storage** - Full history consumes more space  

**When to use this architecture:**
- Enterprise data warehouse
- Multiple changing source systems
- Need for historical accuracy
- Regulatory compliance requirements
- Large, evolving datasets

**For detailed execution steps, see:** [Setup Guide](setup_guide.md)

---

## PART IV: PERFORMANCE & OPTIMIZATION

---

## Query Engine Comparison

### Overview

This project benchmarks **three query engines** on identical datasets to provide empirical performance data for technology selection:

1. **Spark SQL** - General-purpose distributed processing
2. **Hive on Tez** - Optimized DAG-based execution
3. **Apache Impala** - MPP (Massively Parallel Processing) engine

All three engines query the same **Apache Iceberg tables** in the Gold layer, ensuring a fair comparison.

---

### Architecture: Query Engine Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEMANTIC LAYER (SQL Interface)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SPARK SQL     â”‚   â”‚  HIVE ON TEZ    â”‚   â”‚     IMPALA      â”‚
â”‚                 â”‚   â”‚                 â”‚   â”‚                 â”‚
â”‚ â€¢ In-memory     â”‚   â”‚ â€¢ DAG-based     â”‚   â”‚ â€¢ MPP engine    â”‚
â”‚ â€¢ Catalyst      â”‚   â”‚ â€¢ Container     â”‚   â”‚ â€¢ Always-on     â”‚
â”‚   optimizer     â”‚   â”‚   execution     â”‚   â”‚   daemons       â”‚
â”‚ â€¢ Adaptive      â”‚   â”‚ â€¢ YARN resource â”‚   â”‚ â€¢ C++ runtime   â”‚
â”‚   execution     â”‚   â”‚   mgmt          â”‚   â”‚ â€¢ LLVM codegen  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   HIVE METASTORE        â”‚
                    â”‚   (Table Metadata)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   APACHE ICEBERG        â”‚
                    â”‚   (Gold Layer Tables)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   PARQUET FILES         â”‚
                    â”‚   (warehouse/gold/)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Query Engine Architectures

#### Spark SQL Architecture

**Execution Model:** In-memory distributed processing

```
Query â†’ Catalyst Optimizer â†’ Logical Plan â†’ Physical Plan
         â†“                     â†“              â†“
    Rule-based            Cost-based      Adaptive
    optimization          optimization    execution
         â†“                     â†“              â†“
    Tungsten Execution Engine (whole-stage code generation)
         â†“
    Distributed Task Execution (workers process partitions)
```

**Strengths:**
- âœ… Unified engine (batch + streaming + ML)
- âœ… In-memory caching for repeated queries
- âœ… Adaptive Query Execution (AQE) adjusts at runtime
- âœ… Rich ecosystem (MLlib, GraphX, Structured Streaming)

**Weaknesses:**
- âš ï¸ Higher latency for ad-hoc queries (JVM startup overhead)
- âš ï¸ Resource-intensive (requires executors to be running)
- âš ï¸ Complexity in tuning (many configuration parameters)

**Best For:**
- Complex ETL pipelines
- Machine learning workflows
- Mixed batch + interactive workloads
- Data science exploration

---

#### Hive on Tez Architecture

**Execution Model:** DAG-based MapReduce replacement

```
Query â†’ HiveQL Parser â†’ Optimizer â†’ Tez DAG Generator
         â†“                â†“           â†“
    Semantic analysis  Cost-based   Directed Acyclic
                      optimization   Graph (DAG)
         â†“                â†“           â†“
    YARN Container Allocation (on-demand resources)
         â†“
    Tez Runtime (task execution in containers)
```

**Strengths:**
- âœ… Efficient batch processing (better than MapReduce)
- âœ… Cost-effective (spins up/down containers)
- âœ… YARN integration (shared cluster resources)
- âœ… Lower memory footprint than Spark

**Weaknesses:**
- âš ï¸ Higher latency than Impala (container startup time)
- âš ï¸ Not optimized for interactive queries
- âš ï¸ Limited concurrency compared to Impala

**Best For:**
- Large batch ETL jobs
- Cost-sensitive environments (pay-per-use)
- Existing Hadoop ecosystems
- Scheduled reporting workloads

---

#### Apache Impala Architecture

**Execution Model:** Massively Parallel Processing (MPP)

```
Query â†’ Impala Frontend (Java) â†’ Query Planner
         â†“                         â†“
    SQL parsing               Cost-based optimization
         â†“                         â†“
    Impala Backend (C++) â†’ LLVM Code Generation
         â†“                         â†“
    Always-on Daemons       Runtime filters
         â†“                         â†“
    Parallel Execution (all nodes simultaneously)
```

**Strengths:**
- âœ… **Lowest latency** (sub-second for simple queries)
- âœ… High concurrency (100+ concurrent users)
- âœ… C++ runtime (no JVM overhead)
- âœ… LLVM code generation (runtime optimization)
- âœ… Short-circuit reads (local data access)

**Weaknesses:**
- âš ï¸ Memory-intensive (requires dedicated daemons)
- âš ï¸ Limited fault tolerance (in-memory only)
- âš ï¸ Not suitable for ETL (query-only engine)
- âš ï¸ Requires cluster resources 24/7

**Best For:**
- Interactive BI dashboards (Tableau, Power BI)
- Ad-hoc analytics (data exploration)
- High-concurrency environments
- Real-time reporting (SLA < 5 seconds)

---

### Performance Characteristics

#### Query Execution Time Comparison

Based on benchmark results (see [Benchmarking Guide](setup_guide.md#stage-7-query-engine-benchmarking)):

| Query Type | Data Size | Spark SQL | Hive Tez | Impala | Winner |
|------------|-----------|-----------|----------|--------|--------|
| **Simple Aggregation** | 1K rows | 2.5s | 4.8s | 0.7s | Impala (3.6x faster) |
| **Complex Join (3 tables)** | 10K rows | 6.2s | 9.5s | 3.1s | Impala (2x faster) |
| **Temporal Query (PIT)** | 5K rows | 7.8s | 11.2s | 4.3s | Impala (1.8x faster) |
| **Multi-Item Aggregation** | 15K rows | 5.1s | 8.7s | 2.4s | Impala (2.1x faster) |
| **Schema Evolution Query** | 1K rows | 3.9s | 7.1s | 1.5s | Impala (2.6x faster) |

**Key Observations:**
- **Impala** consistently 2-4x faster for interactive queries
- **Spark SQL** shows better performance on complex transformations
- **Hive Tez** is slowest but most cost-effective (lowest resource usage)

---

#### Resource Utilization

| Metric | Spark SQL | Hive Tez | Impala |
|--------|-----------|----------|--------|
| **Memory Footprint** | High (4-8 GB) | Medium (2-4 GB) | High (6-10 GB) |
| **CPU Utilization** | High (multi-core) | Medium (containerized) | Very High (all cores) |
| **Startup Time** | 3-5s (executor init) | 5-8s (container launch) | 0s (always-on) |
| **Concurrent Users** | 10-20 | 5-10 | 50-100+ |
| **Fault Tolerance** | High (RDD lineage) | High (DAG replay) | Low (in-memory only) |

---

#### Scalability Profile

**Spark SQL:**
```
Performance = O(n/p)  where n=data size, p=partitions
Scales horizontally with more executors
Best for: Data volumes > 100 GB
```

**Hive Tez:**
```
Performance = O(n/c)  where n=data size, c=containers
Scales with YARN cluster capacity
Best for: Batch jobs on shared clusters
```

**Impala:**
```
Performance = O(n/d)  where n=data size, d=daemons
Scales with number of always-on nodes
Best for: Data volumes < 1 TB (in-memory limits)
```

---

### Technology Selection Matrix

#### Decision Framework

| Your Requirement | Choose Spark SQL | Choose Hive Tez | Choose Impala |
|------------------|------------------|-----------------|---------------|
| **Latency < 1 second** | âŒ No | âŒ No | âœ… **Yes** |
| **100+ concurrent users** | âš ï¸ Maybe | âŒ No | âœ… **Yes** |
| **Complex ETL + ML** | âœ… **Yes** | âŒ No | âŒ No |
| **Cost optimization** | âš ï¸ Maybe | âœ… **Yes** | âŒ No |
| **Real-time dashboards** | âŒ No | âŒ No | âœ… **Yes** |
| **Large batch jobs (TB+)** | âœ… **Yes** | âœ… **Yes** | âš ï¸ Maybe |
| **Existing Hadoop cluster** | âœ… Yes | âœ… **Yes** | âœ… Yes |
| **Cloud-native (AWS/Azure)** | âœ… **Yes** | âš ï¸ Maybe | âš ï¸ Maybe |

---

### Benchmark Methodology

#### Test Environment

**Hardware:**
- CPU: Intel i7 (8 cores)
- RAM: 16 GB
- Storage: SSD (500 GB)
- OS: Windows 11

**Software Versions:**
- Spark 3.5.0
- Hive 3.1.3 + Tez 0.10.2
- Impala 4.2.0
- Iceberg 1.4.0

**Dataset:**
- 1,000 customers
- 2,000 accounts
- 5,000 transactions (10,000 items)
- Total data: ~500 MB (Gold layer)

---

#### Benchmark Queries

See detailed queries in [Stage 7: Benchmarking](setup_guide.md#stage-7-query-engine-benchmarking)

**Query Categories:**
1. **Simple** - Single table, basic aggregation
2. **Medium** - 2-3 table joins, GROUP BY
3. **Complex** - Multi-table joins, temporal logic, subqueries

**Measurement:**
- **Cold Run** - First execution (no cache)
- **Warm Run** - Second execution (cached metadata)
- **Average** - Mean of 3 iterations

---

### Performance Tuning Tips

#### Spark SQL Optimizations

```scala
// Enable Adaptive Query Execution
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")

// Tune broadcast join threshold
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 10485760) // 10 MB

// Enable dynamic partition pruning
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
```

---

#### Hive Tez Optimizations

```sql
-- Use Tez execution engine
SET hive.execution.engine=tez;

-- Optimize container size
SET hive.tez.container.size=4096;

-- Enable vectorized execution
SET hive.vectorized.execution.enabled=true;

-- Enable cost-based optimizer
SET hive.cbo.enable=true;
SET hive.compute.query.using.stats=true;
```

---

#### Impala Optimizations

```sql
-- Refresh metadata after data changes
INVALIDATE METADATA gold.dim_customer;

-- Compute statistics for cost-based optimization
COMPUTE STATS gold.dim_customer;

-- Use runtime filters for joins
SET RUNTIME_FILTER_MODE=GLOBAL;

-- Increase memory limit for complex queries
SET MEM_LIMIT=8GB;
```

---

### Real-World Use Cases

#### Use Case 1: Interactive BI Dashboard (Tableau)

**Requirement:** Dashboard refresh < 5 seconds, 50 concurrent users

**Best Choice:** **Impala**

**Why:**
- Sub-second query responses
- High concurrency support
- Native JDBC/ODBC connectivity
- Always-on availability

**Benchmark Result:** 2.1s average (vs 6.2s Spark, 9.5s Hive)

---

#### Use Case 2: Nightly ETL Pipeline

**Requirement:** Process 1 TB daily, cost-optimized

**Best Choice:** **Hive on Tez** or **Spark SQL**

**Why:**
- Efficient batch processing
- Scales to large datasets
- YARN resource sharing (Hive) or cloud auto-scaling (Spark)

**Benchmark Result:** Hive Tez uses 40% less memory than Spark

---

#### Use Case 3: Data Science Exploration

**Requirement:** Ad-hoc queries + ML model training

**Best Choice:** **Spark SQL**

**Why:**
- Unified platform (SQL + MLlib + Python)
- In-memory caching for iterative queries
- Jupyter notebook integration

**Benchmark Result:** Spark enables ML workflows that Hive/Impala cannot support

---

### Conclusion

**No single winner** - Choose based on workload:

| Workload Pattern | Recommended Engine |
|------------------|-------------------|
| **Interactive Analytics** | Impala (2-4x faster) |
| **Batch ETL** | Spark SQL or Hive Tez |
| **Mixed Workloads** | Spark SQL (versatility) |
| **Cost Optimization** | Hive Tez (lowest TCO) |
| **Real-time Dashboards** | Impala (sub-second) |

**Hybrid Approach:**
- Use **Impala** for user-facing dashboards
- Use **Spark SQL** for ETL pipelines
- Use **Hive Tez** for scheduled batch jobs

**For detailed execution instructions, see:** [Benchmarking Guide](setup_guide.md#stage-7-query-engine-benchmarking)

---

