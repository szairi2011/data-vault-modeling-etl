###################################################################################
# SPARK CONFIGURATION (LOCAL DEVELOPMENT - WINDOWS NATIVE)
###################################################################################
#
# PURPOSE:
# Configure Spark for local development with Iceberg and Hive Metastore on Windows.
#
# KEY CONCEPTS:
#
# 1. SPARK CATALOG:
#    - Entry point for accessing tables
#    - Delegates to Iceberg/Hive for metadata
#    - Supports multiple catalogs (spark_catalog, dev_catalog, prod_catalog)
#
# 2. ICEBERG CATALOG TYPES:
#    - HiveCatalog: Uses HMS for metadata (our choice)
#    - HadoopCatalog: FileSystem-based (simpler, no HMS)
#    - NessieCatalog: Git-like versioning
#
# 3. SPARK-ICEBERG INTEGRATION:
#    - Iceberg acts as Spark data source (like Parquet/ORC)
#    - Supports DataFrame API and SQL
#    - ACID transactions via optimistic locking
#    - Time travel queries (AS OF version/timestamp)
#
# 4. WRITE MODES:
#    - append: Add new data files (Data Vault default)
#    - overwrite: Replace partition/table
#    - merge: Upsert operations (Iceberg 0.14+)
#
# HOW TO USE THIS FILE:
#   1. Copy to src/main/resources/spark-defaults.conf
#   2. Or specify via --properties-file when launching spark-shell/spark-submit
#   3. Or set in SparkSession.builder().config("key", "value")
#
###################################################################################

# ===== Master Configuration =====
# Run Spark in local mode using all available cores
spark.master                                    local[*]

# local[*] means:
# - Run Spark locally (no cluster)
# - Use all available CPU cores
# - Perfect for development and testing
# - For production: yarn, k8s://..., spark://master:7077

# ===== Application Defaults =====
spark.app.name                                  DataVault-ETL
spark.driver.memory                             4g
spark.executor.memory                           4g

# Adjust memory based on your machine:
# - Laptop (8GB RAM): driver=2g, executor=2g
# - Desktop (16GB RAM): driver=4g, executor=4g
# - Workstation (32GB RAM): driver=8g, executor=8g

# ═════════════════════════════════════════════════════════════════════════════
# CATALOG CONFIGURATION (ICEBERG + HIVE METASTORE)
# ═════════════════════════════════════════════════════════════════════════════

# Set default catalog to use Iceberg with Hive Metastore
spark.sql.catalog.spark_catalog                 org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.spark_catalog.type            hive
spark.sql.catalog.spark_catalog.uri

# WHY EMPTY URI?
# Empty URI = embedded HMS (no separate service)
# HMS runs inside Spark JVM, uses Derby database in metastore_db/
#
# FOR REMOTE HMS (Production):
# spark.sql.catalog.spark_catalog.uri           thrift://hms-server:9083

# Warehouse location (must match hive-site.xml)
spark.sql.catalog.spark_catalog.warehouse       file:///C:/dev/projects/data-vault-modeling-etl/warehouse

# WHY HIVE CATALOG:
# ✅ Industry standard (compatible with Impala, Presto, Trino)
# ✅ Centralized metadata (single source of truth)
# ✅ Multi-engine support (Spark + Impala)
# ✅ Schema evolution tracking
# ✅ Partition pruning optimization

# Alternative: Hadoop Catalog (simpler, but single-engine)
# spark.sql.catalog.spark_catalog.type          hadoop
# spark.sql.catalog.spark_catalog.warehouse     file:///C:/dev/projects/data-vault-modeling-etl/warehouse

# ═════════════════════════════════════════════════════════════════════════════
# ICEBERG EXTENSIONS AND PROPERTIES
# ═════════════════════════════════════════════════════════════════════════════

# Enable Iceberg SQL extensions
spark.sql.extensions                            org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# ICEBERG EXTENSIONS ENABLE:
# - CREATE TABLE ... USING iceberg
# - ALTER TABLE ... ADD/DROP/RENAME COLUMN
# - CALL iceberg.system.rollback_to_snapshot(...)
# - CALL iceberg.system.expire_snapshots(...)
# - Time travel: SELECT * FROM table TIMESTAMP AS OF '2025-01-01'
# - Versioning: SELECT * FROM table VERSION AS OF 123456789

# Default file format for Iceberg tables
spark.sql.iceberg.write.format                  parquet

# PARQUET BENEFITS:
# - Columnar storage (efficient for analytics)
# - Excellent compression (Snappy/Zstd)
# - Predicate pushdown (skip irrelevant data)
# - Schema evolution support
# - Industry standard

# Iceberg table properties (defaults for new tables)
spark.sql.iceberg.compression-codec             snappy
spark.sql.iceberg.parquet.row-group-size-bytes  134217728

# Compression codecs comparison:
# - snappy: Fast, moderate compression (default)
# - gzip: Slower, better compression
# - zstd: Best balance (fast + good compression)
# - lz4: Fastest, least compression

# ═════════════════════════════════════════════════════════════════════════════
# ADAPTIVE QUERY EXECUTION (AQE)
# ═════════════════════════════════════════════════════════════════════════════

spark.sql.adaptive.enabled                      true
spark.sql.adaptive.coalescePartitions.enabled   true
spark.sql.adaptive.skewJoin.enabled             true
spark.sql.adaptive.localShuffleReader.enabled   true

# WHY AQE:
# - Dynamically optimize query plans based on runtime statistics
# - Coalesce small partitions (reduce task overhead)
# - Handle skewed joins (broadcast smaller side)
# - Optimize shuffle reads (convert to local reads when possible)
# - Improve performance without manual tuning

# AQE Examples:
# 1. Coalesce partitions: 200 tiny partitions → 10 optimal partitions
# 2. Skew join: Detect partition with 10x data, split it
# 3. Local shuffle: Avoid network for small joins

# ═════════════════════════════════════════════════════════════════════════════
# COST-BASED OPTIMIZATION (CBO)
# ═════════════════════════════════════════════════════════════════════════════

spark.sql.cbo.enabled                           true
spark.sql.statistics.histogram.enabled          true
spark.sql.cbo.joinReorder.enabled               true

# WHY CBO:
# - Choose optimal join strategy (broadcast vs. shuffle)
# - Predicate pushdown decisions
# - Better partition pruning
# - Join reordering (smallest tables first)

# CBO REQUIRES STATISTICS:
# Collect via: ANALYZE TABLE bronze.hub_customer COMPUTE STATISTICS
# Or auto-collect: hive.stats.autogather=true in hive-site.xml

# ═════════════════════════════════════════════════════════════════════════════
# SHUFFLE CONFIGURATION
# ═════════════════════════════════════════════════════════════════════════════

spark.sql.shuffle.partitions                    200

# TUNING GUIDE:
# - Small datasets (<10GB): 50-100 partitions
# - Medium datasets (10-100GB): 200-500 partitions
# - Large datasets (>100GB): 1000+ partitions
# Rule of thumb: 128MB per partition

# FOR THIS PROJECT (local development):
# - Total data < 1GB: 50 partitions would suffice
# - But 200 is safe default for Spark 3.x

# Calculate optimal partitions:
# partitions = ceiling(total_data_size_mb / 128)

# ═════════════════════════════════════════════════════════════════════════════
# BROADCAST JOIN THRESHOLD
# ═════════════════════════════════════════════════════════════════════════════

spark.sql.autoBroadcastJoinThreshold            268435456

# 256MB (268435456 bytes)
# Tables smaller than this are broadcast to all executors for join
# Avoids expensive shuffle operation

# DATA VAULT BENEFIT:
# Hub tables are typically small (just business keys)
# Perfect candidates for broadcast joins
# Example: Hub_Customer (1M rows) = ~50MB → broadcast
#          Fact_Transaction (100M rows) = 10GB → shuffle
#          Join is fast because Hub is local on each executor

# Adjust based on available memory:
# - Small memory (4GB): 64MB (67108864)
# - Medium memory (8GB): 128MB (134217728)
# - Large memory (16GB+): 256MB (268435456)

# ═════════════════════════════════════════════════════════════════════════════
# DYNAMIC PARTITION OVERWRITE
# ═════════════════════════════════════════════════════════════════════════════

spark.sql.sources.partitionOverwriteMode        dynamic

# DATA VAULT PATTERN: Overwrite only modified partitions
# Example: Reprocess 2025-01-15 without affecting other dates
#
# MODES:
# - static: Overwrites entire table/all partitions (dangerous)
# - dynamic: Overwrites only partitions with data in new write (safe)
#
# EXAMPLE:
# Table partitioned by load_date:
# - 2025-01-13: 1000 rows
# - 2025-01-14: 1000 rows
# - 2025-01-15: 1000 rows
#
# Reprocess 2025-01-15 with 1500 rows:
# - dynamic: Only 2025-01-15 overwritten (13,14 unchanged)
# - static: All dates deleted, only 2025-01-15 remains

# ═════════════════════════════════════════════════════════════════════════════
# TIMEZONE CONFIGURATION
# ═════════════════════════════════════════════════════════════════════════════

spark.sql.session.timeZone                      UTC

# IMPORTANT: Always use UTC for data warehouses
# - Avoids daylight saving time confusion
# - Consistent across regions
# - Standard for international systems
#
# Convert to local timezone in semantic layer:
# SELECT customer_id, CONVERT_TZ(transaction_timestamp, 'UTC', 'America/New_York')
# FROM gold.fact_transaction

# ═════════════════════════════════════════════════════════════════════════════
# ICEBERG TABLE PROPERTIES (DEFAULTS)
# ═════════════════════════════════════════════════════════════════════════════

# Snapshot retention (time travel window)
spark.sql.iceberg.expire-snapshots.min-snapshots-to-keep      10
spark.sql.iceberg.expire-snapshots.max-snapshot-age-ms        604800000

# 7 days retention = 7 * 24 * 60 * 60 * 1000 ms
# Allows time travel up to 1 week back
# Old snapshots auto-deleted by maintenance job:
# CALL spark_catalog.system.expire_snapshots('bronze.hub_customer')

# Manifest file size (affects metadata performance)
spark.sql.iceberg.target-manifest-file-size-bytes             8388608

# 8MB manifests balance metadata size vs. overhead
# Larger = fewer files (better for reads)
# Smaller = more granular (better for writes)

# ═════════════════════════════════════════════════════════════════════════════
# HIVE METASTORE CONFIGURATION
# ═════════════════════════════════════════════════════════════════════════════

# Point to hive-site.xml for HMS configuration
spark.hadoop.hive.metastore.schema.verification                 false
spark.hadoop.datanucleus.autoCreateSchema                       true
spark.hadoop.datanucleus.fixedDatastore                         false

# HMS warehouse directory (must match hive-site.xml)
spark.hadoop.hive.metastore.warehouse.dir                       file:///C:/dev/projects/data-vault-modeling-etl/warehouse

# Enable Hive support in Spark SQL
spark.sql.catalogImplementation                                 hive

# ═════════════════════════════════════════════════════════════════════════════
# WINDOWS-SPECIFIC SETTINGS
# ═════════════════════════════════════════════════════════════════════════════

# Derby home directory (HMS embedded database)
spark.driver.extraJavaOptions                   -Dderby.system.home=C:/dev/projects/data-vault-modeling-etl/metastore_db

# Avoid issues with Windows paths
spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs   false

# Disable Hadoop native libraries (not needed for local mode)
spark.hadoop.io.nativeio.enabled                                false

# ═════════════════════════════════════════════════════════════════════════════
# LOGGING CONFIGURATION
# ═════════════════════════════════════════════════════════════════════════════

# Event log for Spark UI history
spark.eventLog.enabled                          true
spark.eventLog.dir                              file:///C:/dev/projects/data-vault-modeling-etl/logs/spark-events

# View completed applications at http://localhost:18080 (requires history server)
# Start history server: spark-class org.apache.spark.deploy.history.HistoryServer

# Log level (reduce verbosity)
spark.log.level                                 WARN

# Valid levels: ALL, DEBUG, INFO, WARN, ERROR, FATAL, OFF

# ═════════════════════════════════════════════════════════════════════════════
# SERIALIZATION
# ═════════════════════════════════════════════════════════════════════════════

spark.serializer                                org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired                 false

# Kryo is faster than Java serialization (10x in some cases)
# Used for shuffles and caching

# ═════════════════════════════════════════════════════════════════════════════
# UI CONFIGURATION
# ═════════════════════════════════════════════════════════════════════════════

spark.ui.enabled                                true
spark.ui.port                                   4040

# Spark UI available at http://localhost:4040 while job is running
# Shows stages, tasks, SQL queries, storage, executors

###################################################################################
# USAGE EXAMPLES:
###################################################################################

# 1. Create Iceberg table:
#    spark.sql("CREATE TABLE bronze.hub_customer (customer_hash_key STRING, customer_id INT) USING iceberg")

# 2. Append data:
#    df.writeTo("bronze.hub_customer").using("iceberg").append()

# 3. Time travel query:
#    spark.sql("SELECT * FROM bronze.hub_customer TIMESTAMP AS OF '2025-01-15 10:00:00'")

# 4. View snapshots:
#    spark.sql("SELECT * FROM bronze.hub_customer.snapshots")

# 5. Rollback:
#    spark.sql("CALL spark_catalog.system.rollback_to_snapshot('bronze.hub_customer', 6574839201)")

###################################################################################
# TROUBLESHOOTING:
###################################################################################

# Issue: Derby lock error
# Solution: Only one Spark session can use embedded HMS at a time. Close other sessions.

# Issue: Iceberg table not found
# Solution: Verify HMS warehouse dir exists and matches hive-site.xml

# Issue: Out of memory
# Solution: Reduce spark.driver.memory and spark.executor.memory

# Issue: Slow queries
# Solution: Run ANALYZE TABLE to collect statistics for CBO

###################################################################################

